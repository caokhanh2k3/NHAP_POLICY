{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.5.0\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install openai==1.79.0\n",
    "# !pip install tenacity==9.1.2\n",
    "# !pip install tiktoken==0.9.0\n",
    "# !pip install transformers==4.51.3\n",
    "# !pip install pandas==2.2.3\n",
    "# !pip install scikit-learn==1.6.1\n",
    "# !pip install bitsandbytes==0.45.5\n",
    "# !pip install datasets==3.6.0\n",
    "# !pip install sentencepiece==0.2.0\n",
    "# !pip install peft==0.15.2\n",
    "# !pip install evaluate==0.4.3\n",
    "# !pip install trl==0.11.4\n",
    "# !pip install protobuf==6.31.0\n",
    "# !pip install python-dotenv==1.1.0\n",
    "# !pip install pandas_ta\n",
    "# !pip install ollama==0.4.8\n",
    "# !pip install accelerate==1.7.0\n",
    "# !pip install ipywidgets\n",
    "# !pip install pynvml==8.1.7\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c545dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "libs = [\n",
    "    \"numpy\", \"openai\", \"tenacity\", \"tiktoken\", \"transformers\", \"pandas\",\n",
    "    \"scikit-learn\", \"torch\", \"bitsandbytes\", \"datasets\", \"sentencepiece\",\n",
    "    \"peft\", \"evaluate\", \"trl\", \"protobuf\", \"python-dotenv\", \"pandas_ta\",\n",
    "    \"ollama\", \"accelerate\", \"ipywidgets\"\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib}=={version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{lib} not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "print(torch.cuda.is_available())  # Nếu trả về False, CUDA chưa hoạt động\n",
    "print(torch.cuda.device_count())  # Kiểm tra số lượng GPU\n",
    "print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU\n",
    "# print(torch.set_default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f2a1fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: Despite negative news such as the ban on N95 and surgical masks for the general public and criticisms of being a sweatshop, Amazon's fulfillment of 80,000 out of 100,000 jobs to meet increased demand during the pandemic and Jeff Bezos' donation to Feeding America showcase their commitment to community support. Partnering with SXSW for an online festival and planning to provide masks and temperature checks for staff demonstrate adaptability. With news of Canada's agreement with Amazon Canada and BMO Capital Markets' \"Outperform\" rating, there are positive signals. Speculations about Amazon's Prime Day event postponement may have led to short-term stock fluctuations, but the overall sentiment remains optimistic.From a technical perspective, despite the daily stock price range and fluctuation, key indicators such as the ADX and RSI suggest a strong bullish momentum. The contact with coronavirus test makers and anticipation for advanced technological solutions like driverless trucks and drone deliveries align with the market trend towards innovation, favorably impacting Amazon's long-term growth potential. These combined factors indicate a positive price movement outlook for Amazon stock.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'completion', 'prediction', 'explain'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "\n",
    "\n",
    "data_path = \"./datasets/\"\n",
    "\n",
    "ds_dict = load_dataset(data_path)\n",
    "\n",
    "ds_dict\n",
    "\n",
    "# Lấy tập train từ DatasetDict\n",
    "ds = ds_dict[\"train\"]\n",
    "\n",
    "\n",
    "# Tách completion_a và completion_b thành từng dòng mới, rồi tách \"Explanation:\"\n",
    "def split_completion(completion_text):\n",
    "    if \"Explanation:\" in completion_text:\n",
    "        parts = completion_text.split(\"Explanation:\", 1)\n",
    "        target = parts[0].strip()\n",
    "        explain = \"Explanation: \" + parts[1].strip()\n",
    "\n",
    "        explain = explain.replace(\"\\n\", \"\")\n",
    "\n",
    "    else:\n",
    "        target = completion_text.strip()\n",
    "        explain = \"\"\n",
    "    return target, explain\n",
    "\n",
    "\n",
    "# Bước 2: Chuyển đổi từng dòng thành 2 dòng mới\n",
    "new_data = []\n",
    "for example in ds:\n",
    "    prediction_a, explain_a = split_completion(example[\"completion_a\"])\n",
    "    new_data.append({\"user_input\": example[\"user_input\"], \"completion\": example[\"completion_a\"], \"prediction\": prediction_a, \"explain\": explain_a})\n",
    "\n",
    "    prediction_b, explain_b = split_completion(example[\"completion_b\"])\n",
    "    new_data.append({\"user_input\": example[\"user_input\"], \"completion\": example[\"completion_b\"], \"prediction\": prediction_b, \"explain\": explain_b})\n",
    "\n",
    "# Bước 3: Tạo dataset mới từ danh sách đã chuyển đổi\n",
    "new_ds = Dataset.from_list(new_data)\n",
    "\n",
    "# Bước 4: In thử dữ liệu\n",
    "print(new_ds[1]['explain'])\n",
    "new_ds[1]['explain']\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c2c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m new_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m ds:\n\u001b[1;32m----> 4\u001b[0m     new_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_a\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m      5\u001b[0m     new_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_b\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Bước 3: Tạo dataset mới từ danh sách đã chuyển đổi\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer # Ví dụ cho LLMs\n",
    "# from trl import PPOConfig, PPOTrainer # Ví dụ cho PPO\n",
    "# Các thư viện khác cho ODE solver, flow matching có thể cần thiết\n",
    "\n",
    "# --- Hyperparameters and Configurations ---\n",
    "LLM_EXPLANATION_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\" # Ví dụ\n",
    "LLM_GUIDANCE_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"   # Ví dụ\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE_EXPLANATION_LLM = 2e-5\n",
    "LEARNING_RATE_FLOW_MODEL = 2e-4\n",
    "NUM_EPOCHS_FLOW_MODEL = 100\n",
    "NUM_EPOCHS_EXPLANATION_LLM = 10\n",
    "# ... (Thêm các hyperparameters khác)\n",
    "\n",
    "# --- 0. Data Loading and Preprocessing ---\n",
    "def load_dataset(task_name):\n",
    "    # Input: task_name (e.g., \"smac\", \"mmlu_law\")\n",
    "    # Output: list of (context, actual_decision, all_possible_actions)\n",
    "    # Ví dụ:\n",
    "    # if task_name == \"smac\":\n",
    "    #     data = [...] # Tải dữ liệu quỹ đạo SMAC\n",
    "    # elif task_name == \"mmlu_law\":\n",
    "    #     data = [...] # Tải dữ liệu MMLU Luật\n",
    "    # return data\n",
    "    pass\n",
    "\n",
    "# --- 1. LLM Components ---\n",
    "class ExplanationLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "        # Có thể thêm LoRA adapter ở đây nếu dùng\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=LEARNING_RATE_EXPLANATION_LLM)\n",
    "\n",
    "    def generate_explanation(self, context_text):\n",
    "        # Input: context_text (string)\n",
    "        # Output: explanation_sentences (list of strings)\n",
    "        prompt = f\"Given [Context: {context_text}]. Please analyze reasoning for the agent decision based on the context.\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        # Chú ý: Cần logic để chia output thành các câu\n",
    "        # outputs = self.model.generate(**inputs, max_new_tokens=200, ...)\n",
    "        # explanation_full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # explanation_sentences = explanation_full_text.split('.') # Ví dụ đơn giản\n",
    "        # return [s.strip() + \".\" for s in explanation_sentences if s.strip()]\n",
    "        pass\n",
    "\n",
    "    def train_step_ppo(self, contexts, explanations, rewards):\n",
    "        # Input:\n",
    "        #   contexts: list of context strings\n",
    "        #   explanations: list of generated explanation (list of sentences)\n",
    "        #   rewards: list of per-sentence rewards (list of floats)\n",
    "        # Logic huấn luyện PPO sử dụng TRL hoặc triển khai tùy chỉnh\n",
    "        # ppo_trainer.step(query_tensors, response_tensors, reward_tensors)\n",
    "        pass\n",
    "\n",
    "class GuidanceLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "        # LLM Hướng dẫn thường được giữ cố định (frozen) hoặc chỉ tinh chỉnh nhẹ\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_decision_distribution(self, context_text, explanation_sentences_so_far, all_possible_actions):\n",
    "        # Input:\n",
    "        #   context_text: string\n",
    "        #   explanation_sentences_so_far: list of strings (e.g., s_1, or [s_1, s_2])\n",
    "        #   all_possible_actions: list of strings representing decisions\n",
    "        # Output: probability_distribution (torch.Tensor, shape: [num_actions])\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for action in all_possible_actions:\n",
    "                prompt = f\"Given [Context: {context_text}], the reasoning is [{current_explanation}]. Thus, the decision is [{action}].\"\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "                # Lấy logits cho token cuối cùng hoặc trung bình logits của token mô tả action\n",
    "                # outputs = self.model(**inputs)\n",
    "                # action_logits = ... # Cần logic phức tạp để lấy logit cho toàn bộ action\n",
    "                # probs.append(action_logits) # Đây sẽ là logit, không phải prob\n",
    "        # logits_tensor = torch.tensor(probs)\n",
    "        # probability_distribution = torch.softmax(logits_tensor, dim=0)\n",
    "        # return probability_distribution\n",
    "        pass\n",
    "\n",
    "    def get_last_layer_hidden_states(self, context_text, explanation_sentences_so_far, all_possible_actions_text_for_prompt):\n",
    "        # Input: như trên, nhưng all_possible_actions_text_for_prompt là một phần của prompt\n",
    "        # Output: last_hidden_states (torch.Tensor) từ lớp cuối của LLM Hướng dẫn\n",
    "        # Cần để `output_hidden_states=True` khi gọi model\n",
    "        # prompt = f\"Given [Context: {context_text}], the reasoning is [{current_explanation}]. Thus, the decision is {all_possible_actions_text_for_prompt}.\"\n",
    "        # inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        # last_hidden_states = outputs.hidden_states[-1] # Lấy hidden states của lớp cuối cùng\n",
    "        # return last_hidden_states\n",
    "        pass\n",
    "\n",
    "# --- 2. Rectified Flow Model (φ) ---\n",
    "class RectifiedFlowModel(nn.Module):\n",
    "    def __init__(self, guidance_llm_model, num_actions, hidden_dim_guidance_llm, flow_embed_dim=256, projector_layers=4):\n",
    "        super().__init__()\n",
    "        self.guidance_llm_model = guidance_llm_model # Tham chiếu đến mô hình LLM Hướng dẫn (đã load)\n",
    "        self.num_actions = num_actions\n",
    "        self.hidden_dim_guidance_llm = hidden_dim_guidance_llm # Ví dụ: 4096 cho Llama 7B\n",
    "        self.flow_embed_dim = flow_embed_dim\n",
    "\n",
    "        # Embedding cho z_t và PE(t)\n",
    "        self.zt_embed = nn.Sequential(nn.Linear(num_actions, flow_embed_dim), nn.ReLU(), nn.LayerNorm(flow_embed_dim))\n",
    "        self.time_embed = nn.Sequential(nn.Linear(1, flow_embed_dim), nn.ReLU(), nn.LayerNorm(flow_embed_dim))\n",
    "\n",
    "        # Lớp chú ý chéo (Cross-Attention)\n",
    "        # Trọng số W_Q, W_K, W_V thường được lấy/chia sẻ từ lớp cuối của guidance_llm_model\n",
    "        # Đây là phần phức tạp nhất để triển khai đúng.\n",
    "        # self.cross_attention = nn.MultiheadAttention(embed_dim=flow_embed_dim, num_heads=8, kdim=hidden_dim_guidance_llm, vdim=hidden_dim_guidance_llm, batch_first=True)\n",
    "        # Hoặc một triển khai tùy chỉnh để khớp với mô tả trong bài báo:\n",
    "        # Query: từ flow_tokens\n",
    "        # Key/Value: từ flow_tokens + guidance_llm_hidden_states\n",
    "\n",
    "        # Bộ chiếu (Projector)\n",
    "        projector_modules = []\n",
    "        # Input_dim cho projector sẽ là flow_embed_dim (từ h_ATTN_zt) + num_actions (z_t) + 1 (time t) cho skip connections\n",
    "        # current_dim = flow_embed_dim + num_actions + 1\n",
    "        # for _ in range(projector_layers - 1):\n",
    "        #     projector_modules.extend([nn.Linear(current_dim, flow_embed_dim), nn.ReLU(), nn.LayerNorm(flow_embed_dim)])\n",
    "        #     current_dim = flow_embed_dim + num_actions + 1 # Thêm skip connections vào mỗi lớp\n",
    "        # projector_modules.append(nn.Linear(current_dim, num_actions))\n",
    "        # self.projector = nn.Sequential(*projector_modules)\n",
    "        self.projector = nn.Linear(flow_embed_dim, num_actions) # Phiên bản đơn giản hóa\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=LEARNING_RATE_FLOW_MODEL)\n",
    "\n",
    "    def forward(self, z_t, time_t, guidance_llm_last_hidden_states):\n",
    "        # Input:\n",
    "        #   z_t: (batch_size, num_actions) - trạng thái dòng hiện tại\n",
    "        #   time_t: (batch_size, 1) - thời gian ODE\n",
    "        #   guidance_llm_last_hidden_states: (batch_size, seq_len_guidance, hidden_dim_guidance_llm)\n",
    "        # Output: vector_field (batch_size, num_actions) - tức là φ(t, z_t)\n",
    "\n",
    "        h_emb_zt = self.zt_embed(z_t)  # (batch_size, flow_embed_dim)\n",
    "        h_emb_t = self.time_embed(time_t) # (batch_size, flow_embed_dim)\n",
    "\n",
    "        # Flow tokens: (batch_size, 2, flow_embed_dim)\n",
    "        flow_tokens_emb = torch.stack([h_emb_zt, h_emb_t], dim=1)\n",
    "\n",
    "        # --- Cross-Attention Logic ---\n",
    "        # Đây là phần cốt lõi và phức tạp\n",
    "        # Giả sử guidance_llm_last_hidden_states đã được chuẩn bị\n",
    "        # queries_flow = flow_tokens_emb # (B, 2, D_flow)\n",
    "        # keys_guidance = guidance_llm_last_hidden_states # (B, S_guidance, D_guidance)\n",
    "        # values_guidance = guidance_llm_last_hidden_states # (B, S_guidance, D_guidance)\n",
    "\n",
    "        # Lấy trọng số W_Q, W_K, W_V từ LLM Hướng dẫn (cần truy cập vào các tham số của nó)\n",
    "        # W_q_guidance = self.guidance_llm_model.model.layers[-1].self_attn.q_proj.weight\n",
    "        # W_k_guidance = self.guidance_llm_model.model.layers[-1].self_attn.k_proj.weight\n",
    "        # W_v_guidance = self.guidance_llm_model.model.layers[-1].self_attn.v_proj.weight\n",
    "\n",
    "        # Áp dụng W_Q cho flow_tokens\n",
    "        # q_flow_projected = torch.matmul(queries_flow, W_q_guidance.T[:self.flow_embed_dim, :self.flow_embed_dim]) # Cần điều chỉnh chiều\n",
    "\n",
    "        # Áp dụng W_K, W_V cho cả flow_tokens và guidance_llm_last_hidden_states rồi concat\n",
    "        # ... (logic phức tạp để tạo key và value cho cross-attention)\n",
    "\n",
    "        # attn_output, _ = self.cross_attention(q_flow_projected, combined_keys, combined_values)\n",
    "        # h_attn_zt = attn_output[:, 0, :] # Lấy output tương ứng với h_emb_zt\n",
    "\n",
    "        # Phiên bản đơn giản hóa: bỏ qua cross-attention phức tạp, chỉ dùng h_emb_zt\n",
    "        h_attn_zt = h_emb_zt # <<<< ĐƠN GIẢN HÓA ĐỂ MINH HỌA\n",
    "\n",
    "        # --- Projector ---\n",
    "        # Thêm skip connections (phiên bản đơn giản)\n",
    "        # projector_input = torch.cat([h_attn_zt, z_t, time_t], dim=1)\n",
    "        # vector_field = self.projector(projector_input)\n",
    "        vector_field = self.projector(h_attn_zt) # <<<< ĐƠN GIẢN HÓA\n",
    "\n",
    "        return vector_field\n",
    "\n",
    "    def train_step(self, z0_batch, z1_batch, time_batch, guidance_llm_hidden_states_batch):\n",
    "        # Input:\n",
    "        #   z0_batch: (batch_size, num_actions) - nhiễu Gaussian\n",
    "        #   z1_batch: (batch_size, num_actions) - phân phối mục tiêu từ mẫu dương\n",
    "        #   time_batch: (batch_size, 1) - thời gian t ngẫu nhiên trong [0,1]\n",
    "        #   guidance_llm_hidden_states_batch: (batch_size, seq_len, hidden_dim) - từ LLM Hướng dẫn\n",
    "        self.optimizer.zero_grad()\n",
    "        z_t_batch = time_batch * z1_batch + (1.0 - time_batch) * z0_batch # Nội suy tuyến tính\n",
    "        target_vector_field = z1_batch - z0_batch\n",
    "        predicted_vector_field = self.forward(z_t_batch, time_batch, guidance_llm_hidden_states_batch)\n",
    "        loss = nn.MSELoss()(predicted_vector_field, target_vector_field)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def solve_ode_generate_distribution(self, z0, guidance_llm_last_hidden_states, num_steps=10):\n",
    "        # Input:\n",
    "        #   z0: (num_actions) - điểm bắt đầu nhiễu\n",
    "        #   guidance_llm_last_hidden_states: (seq_len, hidden_dim)\n",
    "        #   num_steps: số bước để giải ODE\n",
    "        # Output: z1_hat (num_actions) - phân phối quyết định ước tính\n",
    "        # Sử dụng Euler method hoặc ODE solver tốt hơn\n",
    "        # z_current = z0.unsqueeze(0) # Thêm batch_dim\n",
    "        # dt = 1.0 / num_steps\n",
    "        # for i in range(num_steps):\n",
    "        #     time_t = torch.full((1,1), i * dt, device=DEVICE)\n",
    "        #     with torch.no_grad():\n",
    "        #         vector_field = self.forward(z_current, time_t, guidance_llm_last_hidden_states.unsqueeze(0))\n",
    "        #     z_current = z_current + vector_field * dt\n",
    "        # return torch.softmax(z_current.squeeze(0), dim=0) # Trả về dạng xác suất\n",
    "        pass\n",
    "\n",
    "# --- 3. Main Training Loop ---\n",
    "def main_training_loop(dataset, num_rounds=2):\n",
    "    # Khởi tạo các mô hình\n",
    "    explanation_llm = ExplanationLLM(LLM_EXPLANATION_MODEL_NAME)\n",
    "    guidance_llm = GuidanceLLM(LLM_GUIDANCE_MODEL_NAME) # Dùng để lấy hidden states và tạo mẫu dương ban đầu\n",
    "\n",
    "    # Lấy thông tin cần thiết từ guidance_llm cho rectified_flow_model\n",
    "    # dummy_input_ids = guidance_llm.tokenizer(\"test\", return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
    "    # with torch.no_grad():\n",
    "    #     dummy_outputs = guidance_llm.model(dummy_input_ids, output_hidden_states=True)\n",
    "    # hidden_dim_guidance = dummy_outputs.hidden_states[-1].shape[-1]\n",
    "    # num_actions_example = len(dataset[0][2]) # Lấy số action từ mẫu đầu tiên\n",
    "\n",
    "    # rectified_flow_model = RectifiedFlowModel(\n",
    "    #     guidance_llm_model=guidance_llm.model, # Truyền tham chiếu đến mô hình đã load\n",
    "    #     num_actions=num_actions_example,\n",
    "    #     hidden_dim_guidance_llm=hidden_dim_guidance,\n",
    "    #     # ...\n",
    "    # ).to(DEVICE)\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"--- Round {round_num + 1} ---\")\n",
    "\n",
    "        # --- Giai đoạn 1 (Biến thể): Tạo Mẫu Dương cho Flow Model ---\n",
    "        # (Trong vòng đầu tiên, chúng ta dùng πg. Các vòng sau có thể dùng πg hoặc thậm chí φ đã cải thiện)\n",
    "        positive_samples_for_flow = [] # list of (z0, z1, context_for_guidance, explanation_for_guidance)\n",
    "        print(\"Generating positive samples for Flow Model...\")\n",
    "        # for context, actual_decision, all_actions in dataset:\n",
    "            # explanation_initial = explanation_llm.generate_explanation(context) # Có thể không cần ở bước này\n",
    "            # explanation_sentences = [\"Placeholder explanation sentence 1.\", \"Placeholder for positive sample gen.\"] # Hoặc dùng giải thích \"vàng\" nếu có\n",
    "\n",
    "            # prob_dist_from_guidance = guidance_llm.get_decision_distribution(context, explanation_sentences, all_actions)\n",
    "            # actual_decision_idx = all_actions.index(actual_decision)\n",
    "\n",
    "            # if torch.argmax(prob_dist_from_guidance) == actual_decision_idx: # Đây là mẫu dương\n",
    "            #     z1 = prob_dist_from_guidance.detach().cpu()\n",
    "            #     z0 = torch.randn_like(z1)\n",
    "                # guidance_hidden_states = guidance_llm.get_last_layer_hidden_states(context, explanation_sentences, \" \".join(all_actions))\n",
    "                # positive_samples_for_flow.append((z0, z1, guidance_hidden_states.detach().cpu()))\n",
    "        # print(f\"Generated {len(positive_samples_for_flow)} positive samples.\")\n",
    "\n",
    "        # --- Giai đoạn 2: Huấn luyện Rectified Flow Model (φ) ---\n",
    "        print(\"Training Rectified Flow Model (φ)...\")\n",
    "        # for epoch in range(NUM_EPOCHS_FLOW_MODEL):\n",
    "            # total_flow_loss = 0\n",
    "            # for z0_sample, z1_sample, guidance_hs_sample in positive_samples_for_flow: # Cần batching\n",
    "            #     time_sample = torch.rand(1,1).to(DEVICE) # (1,1)\n",
    "            #     loss = rectified_flow_model.train_step(\n",
    "            #         z0_sample.unsqueeze(0).to(DEVICE),\n",
    "            #         z1_sample.unsqueeze(0).to(DEVICE),\n",
    "            #         time_sample,\n",
    "            #         guidance_hs_sample.unsqueeze(0).to(DEVICE)\n",
    "            #     )\n",
    "            #     total_flow_loss += loss\n",
    "            # print(f\"Flow Model Epoch {epoch+1}, Avg Loss: {total_flow_loss / len(positive_samples_for_flow)}\")\n",
    "\n",
    "        # --- Giai đoạn 3: Huấn luyện EXPLANATION LLM (πε) bằng PPO ---\n",
    "        print(\"Training EXPLANATION LLM (πε) with PPO...\")\n",
    "        # for epoch_expl in range(NUM_EPOCHS_EXPLANATION_LLM):\n",
    "            # ppo_batch_contexts = []\n",
    "            # ppo_batch_explanations_text = [] # Dạng text để PPO tokenizer xử lý\n",
    "            # ppo_batch_rewards = []\n",
    "\n",
    "            # for context, actual_decision, all_actions in dataset: # Cần batching\n",
    "            #     explanation_sentences = explanation_llm.generate_explanation(context)\n",
    "            #     per_sentence_rewards_for_ppo = []\n",
    "            #     prev_p_hat_actual_decision = 0.0 # Hoặc giá trị từ phân phối đồng nhất\n",
    "\n",
    "            #     cumulative_explanation = []\n",
    "            #     for sent_idx, sentence in enumerate(explanation_sentences):\n",
    "            #         cumulative_explanation.append(sentence)\n",
    "                        # Lấy hidden states từ LLM Hướng dẫn cho ngữ cảnh và giải thích hiện tại\n",
    "            #             current_guidance_hidden_states = guidance_llm.get_last_layer_hidden_states(\n",
    "            #                 context, cumulative_explanation, \" \".join(all_actions)\n",
    "            #             ).to(DEVICE)\n",
    "\n",
    "                        # Sử dụng φ để tạo phân phối quyết định ước tính p_hat\n",
    "            #             z0_for_phi_inference = torch.randn(len(all_actions)).to(DEVICE)\n",
    "            #             p_hat_distribution = rectified_flow_model.solve_ode_generate_distribution(\n",
    "            #                 z0_for_phi_inference, current_guidance_hidden_states\n",
    "            #             )\n",
    "\n",
    "            #             actual_decision_idx = all_actions.index(actual_decision)\n",
    "            #             p_hat_actual_decision = p_hat_distribution[actual_decision_idx].item()\n",
    "\n",
    "            #             reward_for_sentence = p_hat_actual_decision - prev_p_hat_actual_decision\n",
    "            #             per_sentence_rewards_for_ppo.append(reward_for_sentence)\n",
    "            #             prev_p_hat_actual_decision = p_hat_actual_decision\n",
    "\n",
    "                # Chuẩn bị dữ liệu cho PPO trainer của TRL\n",
    "                # query_text = f\"Context: {context}\" # Hoặc prompt ban đầu của ExplanationLLM\n",
    "                # response_text = \" \".join(explanation_sentences)\n",
    "                # ppo_batch_contexts.append(query_text)\n",
    "                # ppo_batch_explanations_text.append(response_text)\n",
    "                # ppo_batch_rewards.append(torch.tensor(per_sentence_rewards_for_ppo[-1])) # PPO của TRL thường nhận 1 reward cho cả response\n",
    "\n",
    "            # Huấn luyện Explanation LLM bằng PPO (ví dụ với TRL)\n",
    "            # stats = ppo_trainer.step(tokenized_queries, tokenized_responses, rewards_for_ppo_trainer)\n",
    "            # print(f\"Explanation LLM Epoch {epoch_expl+1}, PPO Mean Reward: {stats['ppo/returns/mean']}\")\n",
    "    pass\n",
    "\n",
    "# --- Chạy chương trình ---\n",
    "if __name__ == \"__main__\":\n",
    "    # smac_data = load_dataset(\"smac\")\n",
    "    # main_training_loop(smac_data)\n",
    "    print(\"Khung sườn mã cho Policy-to-Language. Cần triển khai chi tiết.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
