{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "Successfully installed setuptools-80.9.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.5.0\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install openai==1.79.0\n",
    "# !pip install tenacity==9.1.2\n",
    "# !pip install tiktoken==0.9.0\n",
    "# !pip install transformers==4.51.3\n",
    "# !pip install pandas==2.2.3\n",
    "# !pip install datasets==3.6.0\n",
    "# !pip install sentencepiece==0.2.0\n",
    "# !pip install peft==0.15.2\n",
    "# !pip install evaluate==0.4.3\n",
    "# !pip install trl==0.11.4\n",
    "# !pip install protobuf==6.31.0\n",
    "# !pip install python-dotenv==1.1.0\n",
    "# !pip install accelerate==1.7.0\n",
    "# !pip install ipywidgets\n",
    "# !pip install pynvml==8.1.7\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c545dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.26.4\n",
      "openai==1.79.0\n",
      "tenacity==9.1.2\n",
      "tiktoken==0.9.0\n",
      "transformers==4.51.3\n",
      "pandas==2.2.3\n",
      "scikit-learn not installed\n",
      "torch==2.5.0+cu124\n",
      "bitsandbytes not installed\n",
      "datasets==3.6.0\n",
      "sentencepiece==0.2.0\n",
      "peft==0.15.2\n",
      "evaluate==0.4.3\n",
      "trl==0.11.4\n",
      "protobuf==6.31.0\n",
      "python-dotenv==1.1.0\n",
      "pandas_ta not installed\n",
      "ollama not installed\n",
      "accelerate==1.7.0\n",
      "ipywidgets==8.1.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3885/316775683.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "libs = [\n",
    "    \"numpy\", \"openai\", \"tenacity\", \"tiktoken\", \"transformers\", \"pandas\",\n",
    "    \"scikit-learn\", \"torch\", \"bitsandbytes\", \"datasets\", \"sentencepiece\",\n",
    "    \"peft\", \"evaluate\", \"trl\", \"protobuf\", \"python-dotenv\", \"pandas_ta\",\n",
    "    \"ollama\", \"accelerate\", \"ipywidgets\"\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib}=={version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{lib} not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78a4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GRID P40-24Q\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "print(torch.cuda.is_available())  # Nếu trả về False, CUDA chưa hoạt động\n",
    "print(torch.cuda.device_count())  # Kiểm tra số lượng GPU\n",
    "print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU\n",
    "# print(torch.set_default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2a1fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: Despite negative news such as the ban on N95 and surgical masks for the general public and criticisms of being a sweatshop, Amazon's fulfillment of 80,000 out of 100,000 jobs to meet increased demand during the pandemic and Jeff Bezos' donation to Feeding America showcase their commitment to community support. Partnering with SXSW for an online festival and planning to provide masks and temperature checks for staff demonstrate adaptability. With news of Canada's agreement with Amazon Canada and BMO Capital Markets' \"Outperform\" rating, there are positive signals. Speculations about Amazon's Prime Day event postponement may have led to short-term stock fluctuations, but the overall sentiment remains optimistic.From a technical perspective, despite the daily stock price range and fluctuation, key indicators such as the ADX and RSI suggest a strong bullish momentum. The contact with coronavirus test makers and anticipation for advanced technological solutions like driverless trucks and drone deliveries align with the market trend towards innovation, favorably impacting Amazon's long-term growth potential. These combined factors indicate a positive price movement outlook for Amazon stock.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'completion', 'prediction', 'explain'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "\n",
    "\n",
    "data_path = \"./datasets/\"\n",
    "\n",
    "ds_dict = load_dataset(data_path)\n",
    "\n",
    "ds_dict\n",
    "\n",
    "# Lấy tập train từ DatasetDict\n",
    "ds = ds_dict[\"train\"]\n",
    "\n",
    "\n",
    "# Tách completion_a và completion_b thành từng dòng mới, rồi tách \"Explanation:\"\n",
    "def split_completion(completion_text):\n",
    "    if \"Explanation:\" in completion_text:\n",
    "        parts = completion_text.split(\"Explanation:\", 1)\n",
    "        target = parts[0].strip()\n",
    "        explain = \"Explanation: \" + parts[1].strip()\n",
    "\n",
    "        explain = explain.replace(\"\\n\", \"\")\n",
    "\n",
    "    else:\n",
    "        target = completion_text.strip()\n",
    "        explain = \"\"\n",
    "    return target, explain\n",
    "\n",
    "\n",
    "# Bước 2: Chuyển đổi từng dòng thành 2 dòng mới\n",
    "new_data = []\n",
    "for example in ds:\n",
    "    prediction_a, explain_a = split_completion(example[\"completion_a\"])\n",
    "    new_data.append({\"user_input\": example[\"user_input\"], \"completion\": example[\"completion_a\"], \"prediction\": prediction_a, \"explain\": explain_a})\n",
    "\n",
    "    prediction_b, explain_b = split_completion(example[\"completion_b\"])\n",
    "    new_data.append({\"user_input\": example[\"user_input\"], \"completion\": example[\"completion_b\"], \"prediction\": prediction_b, \"explain\": explain_b})\n",
    "\n",
    "# Bước 3: Tạo dataset mới từ danh sách đã chuyển đổi\n",
    "new_ds = Dataset.from_list(new_data)\n",
    "\n",
    "# Bước 4: In thử dữ liệu\n",
    "print(new_ds[1]['explain'])\n",
    "new_ds[1]['explain']\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_GUIDANCE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45113a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034d684",
   "metadata": {},
   "source": [
    "# Guidance and FlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac84a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchdiffeq import odeint  # Cần cài đặt: pip install torchdiffeq\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Thiết lập seed\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LLM_GUIDANCE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "MAX_SEQ_LENGTH_GUIDANCE = 1024\n",
    "LEARNING_RATE_FLOW_MODEL = 2e-4\n",
    "\n",
    "# Lớp Guidance LLM hướng đối tượng\n",
    "class GuidanceLLM:\n",
    "    def __init__(self, model_name=LLM_GUIDANCE_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.hidden_dim = None\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Tải mô hình và tokenizer\"\"\"\n",
    "        print(f\"Loading Guidance LLM: {self.model_name}...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.model.eval()\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "\n",
    "            self.hidden_dim = self.model.config.hidden_size\n",
    "            print(f\"Guidance LLM loaded on {DEVICE}. Hidden dim: {self.hidden_dim}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_decision_distribution(self, user_input_text, explanation_sentences_so_far, all_possible_actions):\n",
    "        \"\"\"Tính phân phối xác suất cho các hành động\"\"\"\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        action_logits_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for action_text in all_possible_actions:\n",
    "                prompt = (\n",
    "                    f\"Given the following information about a company: '{user_input_text}'.\\n\"\n",
    "                    f\"Based on the reasoning: '{current_explanation}'.\\n\"\n",
    "                    f\"Predict the price impact: {action_text}.\"\n",
    "                )\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_SEQ_LENGTH_GUIDANCE\n",
    "                ).to(DEVICE)\n",
    "\n",
    "                try:\n",
    "                    outputs = self.model(**inputs)\n",
    "                    logits = outputs.logits[:, -1, :]  # Logits token cuối\n",
    "                    action_tokens = self.tokenizer(action_text, add_special_tokens=False).input_ids\n",
    "                    if not action_tokens:\n",
    "                        action_logits_list.append(torch.tensor(-10.0).to(DEVICE))\n",
    "                        continue\n",
    "\n",
    "                    # Giả lập: Trung bình logits (cần fine-tune thực tế)\n",
    "                    logit_score = logits.mean()\n",
    "                    action_logits_list.append(logit_score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing action '{action_text}': {e}\")\n",
    "                    action_logits_list.append(torch.tensor(-10.0).to(DEVICE))\n",
    "\n",
    "        if not action_logits_list or all(l.item() == -10.0 for l in action_logits_list):\n",
    "            return torch.ones(len(all_possible_actions), device=DEVICE) / len(all_possible_actions)\n",
    "\n",
    "        logits_tensor = torch.stack(action_logits_list)\n",
    "        probability_distribution = torch.softmax(logits_tensor, dim=0)\n",
    "        return probability_distribution\n",
    "\n",
    "    def get_last_layer_hidden_states(self, user_input_text, explanation_sentences_so_far, all_possible_actions_text_for_prompt):\n",
    "        \"\"\"Lấy hidden states từ tầng cuối\"\"\"\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        prompt = (\n",
    "            f\"Given the following information about a company: '{user_input_text}'.\\n\"\n",
    "            f\"Based on the reasoning: '{current_explanation}'.\\n\"\n",
    "            f\"Consider the possible outcomes: {all_possible_actions_text_for_prompt}.\"\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH_GUIDANCE\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        return outputs.hidden_states[-1]  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "# Mô hình Rectified Flow\n",
    "class RectifiedFlowModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        guidance_llm_instance,  # Instance của GuidanceLLM\n",
    "        num_actions,\n",
    "        flow_embed_dim=256,\n",
    "        projector_hidden_dim=256,\n",
    "        projector_layers=4,\n",
    "        num_attention_heads=4,\n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.guidance_llm_instance = guidance_llm_instance\n",
    "        self.num_actions = num_actions\n",
    "        self.flow_embed_dim = flow_embed_dim\n",
    "        self.projector_hidden_dim = projector_hidden_dim\n",
    "        self.projector_layers = projector_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # Kích thước hidden state từ Guidance LLM\n",
    "        self.hidden_dim_guidance_llm = self.guidance_llm_instance.hidden_dim\n",
    "\n",
    "        # Embedding cho z_t và t\n",
    "        self.zt_embed_mlp = nn.Sequential(\n",
    "            nn.Linear(num_actions, flow_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(flow_embed_dim)\n",
    "        )\n",
    "        self.time_embed_mlp = nn.Sequential(\n",
    "            nn.Linear(1, flow_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(flow_embed_dim)\n",
    "        )\n",
    "\n",
    "        # Truy cập W_Q, W_K, W_V từ Guidance LLM\n",
    "        try:\n",
    "            # Qwen-1.5B: transformer.h[-1].attn.q_proj/k_proj/v_proj\n",
    "            last_guidance_layer_attn = self.guidance_llm_instance.model.transformer.h[-1].attn\n",
    "            self.guidance_wq = last_guidance_layer_attn.q_proj\n",
    "            self.guidance_wk = last_guidance_layer_attn.k_proj\n",
    "            self.guidance_wv = last_guidance_layer_attn.v_proj\n",
    "            print(\"Successfully accessed Q, K, V projection weights from Guidance LLM.\")\n",
    "\n",
    "            # Đóng băng trọng số\n",
    "            for param in [self.guidance_wq.parameters(), self.guidance_wk.parameters(), self.guidance_wv.parameters()]:\n",
    "                for p in param:\n",
    "                    p.requires_grad = False\n",
    "            self.using_borrowed_weights = True\n",
    "        except AttributeError as e:\n",
    "            print(f\"Warning: Could not access Q, K, V weights ({e}). Using new Linear layers.\")\n",
    "            self.flow_qkv_proj = nn.Linear(flow_embed_dim, self.hidden_dim_guidance_llm)\n",
    "            self.guidance_kv_proj = nn.Linear(self.hidden_dim_guidance_llm, self.hidden_dim_guidance_llm)\n",
    "            self.using_borrowed_weights = False\n",
    "\n",
    "        # Cross-attention\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=self.hidden_dim_guidance_llm,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attn_output_norm = nn.LayerNorm(self.hidden_dim_guidance_llm)\n",
    "        self.attn_output_projection = nn.Linear(self.hidden_dim_guidance_llm, flow_embed_dim)\n",
    "\n",
    "        # Projector MLP\n",
    "        projector_modules = []\n",
    "        current_dim = flow_embed_dim\n",
    "        for i in range(projector_layers):\n",
    "            output_dim = projector_hidden_dim if i < projector_layers - 1 else num_actions\n",
    "            projector_modules.append(nn.Linear(current_dim, output_dim))\n",
    "            if i < projector_layers - 1:\n",
    "                projector_modules.extend([\n",
    "                    nn.ReLU(),\n",
    "                    nn.LayerNorm(output_dim),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                ])\n",
    "                current_dim = output_dim\n",
    "        self.projector_mlp = nn.Sequential(*projector_modules)\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=LEARNING_RATE_FLOW_MODEL)\n",
    "\n",
    "    def _project_for_attention(self, tensor, projection_type):\n",
    "        \"\"\"Áp dụng phép chiếu Q, K, V\"\"\"\n",
    "        if self.using_borrowed_weights:\n",
    "            if projection_type == 'q': return self.guidance_wq(tensor)\n",
    "            elif projection_type == 'k': return self.guidance_wk(tensor)\n",
    "            elif projection_type == 'v': return self.guidance_wv(tensor)\n",
    "        else:\n",
    "            if projection_type == 'q': return self.flow_qkv_proj(tensor)\n",
    "            elif projection_type in ['k', 'v']: return self.guidance_kv_proj(tensor)\n",
    "        raise ValueError(f\"Unknown projection_type: {projection_type}\")\n",
    "\n",
    "    def forward(self, z_t, time_t, guidance_llm_last_hidden_states):\n",
    "        batch_size = z_t.shape[0]\n",
    "\n",
    "        # Embedding z_t và t\n",
    "        h_emb_zt = self.zt_embed_mlp(z_t)  # [batch_size, flow_embed_dim]\n",
    "        h_emb_t = self.time_embed_mlp(time_t)  # [batch_size, flow_embed_dim]\n",
    "        flow_tokens_emb = (h_emb_zt + h_emb_t).unsqueeze(1)  # [batch_size, 1, flow_embed_dim]\n",
    "\n",
    "        # Cross-attention\n",
    "        query = self._project_for_attention(flow_tokens_emb, 'q')  # [batch_size, 1, hidden_dim_guidance_llm]\n",
    "        key = self._project_for_attention(guidance_llm_last_hidden_states, 'k')  # [batch_size, seq_len, hidden_dim_guidance_llm]\n",
    "        value = self._project_for_attention(guidance_llm_last_hidden_states, 'v')  # [batch_size, seq_len, hidden_dim_guidance_llm]\n",
    "\n",
    "        attn_output, _ = self.attention_layer(query, key, value, need_weights=False)\n",
    "        attn_output_normalized = self.attn_output_norm(attn_output)\n",
    "        h_attn_zt = self.attn_output_projection(attn_output_normalized.squeeze(1))  # [batch_size, flow_embed_dim]\n",
    "\n",
    "        # Projector\n",
    "        vector_field = self.projector_mlp(h_attn_zt)  # [batch_size, num_actions]\n",
    "        return vector_field\n",
    "\n",
    "    def train_step(self, z0_batch, z1_batch, time_batch, guidance_llm_hidden_states_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        z_t_batch = time_batch * z1_batch + (1.0 - time_batch) * z0_batch\n",
    "        target_vector_field = z1_batch - z0_batch\n",
    "        predicted_vector_field = self.forward(z_t_batch, time_batch, guidance_llm_hidden_states_batch)\n",
    "        loss = nn.MSELoss()(predicted_vector_field, target_vector_field)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def solve_ode_generate_distribution(self, z0, guidance_llm_last_hidden_states, num_steps=10, return_logits=False):\n",
    "        self.eval()\n",
    "        if z0.ndim == 1: z0 = z0.unsqueeze(0)\n",
    "        if guidance_llm_last_hidden_states.ndim == 2: guidance_llm_last_hidden_states = guidance_llm_last_hidden_states.unsqueeze(0)\n",
    "        batch_size = z0.shape[0]\n",
    "\n",
    "        def ode_func(t, z):\n",
    "            time_t = torch.full((batch_size, 1), t, device=DEVICE)\n",
    "            return self.forward(z, time_t, guidance_llm_last_hidden_states)\n",
    "\n",
    "        t = torch.linspace(0, 1, num_steps, device=DEVICE)\n",
    "        z_t = odeint(ode_func, z0, t, method='rk4')  # Runge-Kutta\n",
    "        z1_hat = z_t[-1]\n",
    "        output = z1_hat.squeeze(0) if batch_size == 1 else z1_hat\n",
    "        self.train()\n",
    "        return output if return_logits else torch.softmax(output, dim=-1)\n",
    "\n",
    "# --- Test tích hợp ---\n",
    "if __name__ == '__main__':\n",
    "    # Khởi tạo Guidance LLM\n",
    "    guidance_llm = GuidanceLLM()\n",
    "\n",
    "    # Dữ liệu mẫu\n",
    "    sample_data = {\n",
    "        'user_input': \"Công ty Alpha vừa ra mắt sản phẩm mới X được thị trường đón nhận nồng nhiệt. Doanh số dự kiến tăng 30% trong quý tới. Tuy nhiên, chi phí R&D cho sản phẩm Y đang tăng cao.\",\n",
    "        'prediction': \"positive\",\n",
    "        'explain': \"Sản phẩm X thành công rực rỡ là yếu tố chính. Nó bù đắp được lo ngại về chi phí R&D cho Y. Triển vọng tăng trưởng doanh số rất khả quan.\"\n",
    "    }\n",
    "    POSSIBLE_ACTIONS = [\"negative\", \"positive\"]\n",
    "    correct_action = sample_data['prediction']\n",
    "    correct_action_idx = POSSIBLE_ACTIONS.index(correct_action)\n",
    "\n",
    "    # Tách giải thích\n",
    "    explanation_sentences = [s.strip() for s in sample_data['explain'].split('.') if s.strip()]\n",
    "    print(\"Explanation Sentences:\", explanation_sentences)\n",
    "\n",
    "    # Khởi tạo Rectified Flow\n",
    "    flow_model = RectifiedFlowModel(\n",
    "        guidance_llm_instance=guidance_llm,\n",
    "        num_actions=len(POSSIBLE_ACTIONS),\n",
    "        flow_embed_dim=128,\n",
    "        projector_hidden_dim=128,\n",
    "        num_attention_heads=2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Test Guidance LLM\n",
    "    prob_distributions = []\n",
    "    hidden_states_list = []\n",
    "    for k in range(len(explanation_sentences) + 1):\n",
    "        current_explanation = explanation_sentences[:k]\n",
    "        prob_dist = guidance_llm.get_decision_distribution(\n",
    "            sample_data['user_input'], current_explanation, POSSIBLE_ACTIONS\n",
    "        )\n",
    "        hidden_states = guidance_llm.get_last_layer_hidden_states(\n",
    "            sample_data['user_input'], current_explanation, \", \".join(POSSIBLE_ACTIONS)\n",
    "        )\n",
    "        prob_distributions.append(prob_dist)\n",
    "        hidden_states_list.append(hidden_states)\n",
    "        print(f\"\\nProbability Distribution after {k} sentences: {prob_dist.tolist()}\")\n",
    "        print(f\"  Negative: {prob_dist[0]:.4f}, Positive: {prob_dist[1]:.4f}\")\n",
    "        print(f\"  Hidden States Shape: {hidden_states.shape}\")\n",
    "\n",
    "    # Test Rectified Flow\n",
    "    batch_size = 1\n",
    "    z0_test = torch.randn(batch_size, len(POSSIBLE_ACTIONS)).to(DEVICE)\n",
    "    time_test = torch.rand(batch_size, 1).to(DEVICE)\n",
    "    guidance_hs_test = hidden_states_list[-1]  # Hidden states từ câu cuối\n",
    "    vector_field = flow_model(z0_test, time_test, guidance_hs_test)\n",
    "    print(f\"\\nVector Field Shape: {vector_field.shape}\")\n",
    "    assert vector_field.shape == (batch_size, len(POSSIBLE_ACTIONS))\n",
    "\n",
    "    # Test ODE solver\n",
    "    p_hat_dist = flow_model.solve_ode_generate_distribution(z0_test, guidance_hs_test, num_steps=5)\n",
    "    print(f\"p_hat Distribution: {p_hat_dist.shape}, Sum: {p_hat_dist.sum()}\")\n",
    "    assert p_hat_dist.shape == (batch_size, len(POSSIBLE_ACTIONS))\n",
    "    assert torch.isclose(p_hat_dist.sum(), torch.tensor(1.0, device=DEVICE), atol=1e-5)\n",
    "\n",
    "    # Tính phần thưởng\n",
    "    flow_distributions = []\n",
    "    for k in range(len(explanation_sentences) + 1):\n",
    "        flow_dist = flow_model.solve_ode_generate_distribution(z0_test, hidden_states_list[k], num_steps=5)\n",
    "        flow_distributions.append(flow_dist)\n",
    "    rewards = [flow_distributions[k][correct_action_idx] - flow_distributions[k-1][correct_action_idx] \n",
    "               for k in range(1, len(flow_distributions))]\n",
    "    print(f\"\\nRewards: {rewards}\")\n",
    "\n",
    "    print(\"Integration tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f6d4c",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from torchdiffeq import odeint\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Thiết lập seed\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LLM_MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"  # Thay DeepSeek-R-1.5B\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LEARNING_RATE_FLOW_MODEL = 2e-4\n",
    "\n",
    "# Lớp Explanation LLM\n",
    "class ExplanationLLM:\n",
    "    def __init__(self, model_name=LLM_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        print(f\"Loading Explanation LLM: {self.model_name}...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True\n",
    "            ).to(DEVICE)\n",
    "            self.model.eval()\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "            print(f\"Explanation LLM loaded on {DEVICE}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_explanation(self, context):\n",
    "        \"\"\"Tạo giải thích cho ngữ cảnh\"\"\"\n",
    "        prompt = f\"Analyze: {context}. Provide reasoning without predicting outcome.\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=100,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        explanation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return [s.strip() for s in explanation.split(\". \") if s.strip()]\n",
    "\n",
    "# Lớp Guidance LLM\n",
    "class GuidanceLLM:\n",
    "    def __init__(self, model_name=LLM_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.hidden_dim = None\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        print(f\"Loading Guidance LLM: {self.model_name}...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True\n",
    "            ).to(DEVICE)\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.model.eval()\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "            self.hidden_dim = self.model.config.hidden_size\n",
    "            print(f\"Guidance LLM loaded on {DEVICE}. Hidden dim: {self.hidden_dim}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_decision_distribution(self, user_input_text, explanation_sentences_so_far, all_possible_actions):\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        action_logits_list = []\n",
    "        with torch.no_grad():\n",
    "            for action_text in all_possible_actions:\n",
    "                prompt = (\n",
    "                    f\"Given the following information about a company: '{user_input_text}'.\\n\"\n",
    "                    f\"Based on the reasoning: '{current_explanation}'.\\n\"\n",
    "                    f\"Predict the price impact: {action_text}.\"\n",
    "                )\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_SEQ_LENGTH\n",
    "                ).to(DEVICE)\n",
    "                try:\n",
    "                    outputs = self.model(**inputs)\n",
    "                    logits = outputs.logits[:, -1, :]\n",
    "                    action_tokens = self.tokenizer.action_text, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                    if not action_tokens:\n",
    "                        action_logits_list.append(torch.tensor(-10.0).to(DEVICE))\n",
    "                        continue\n",
    "                    logit_score = logits.mean()  # Giả lập, cần fine-tune\n",
    "                    action_logits_list.append(logit_score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing action '{action_text}': {e}\")\n",
    "                    action_logits_list.append(torch.tensor(-10.0).to(device))\n",
    "        if not action_logits_list or all(l.item() == -10.0 for l in action_logits_list):\n",
    "            return torch.ones(len(all_possible_actions), device=DEVICE) / len(all_possible_actions))\n",
    "        logits_tensor = torch.stack(action_logits_list)\n",
    "        probability_distribution = torch.softmax(logits_tensor, dim=0)\n",
    "        return probability_distribution\n",
    "\n",
    "    def get_last_layer_hidden_states(self, user_input_text, explanation_sentences_so_far, all_possible_actions_text_for_prompt):\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        prompt = (\n",
    "            f\"Given the following information about a company: '{user_input_text}'.\\n\"\n",
    "            f\"Based on the reasoning: '{current_explanation}'.\\n\"\n",
    "            f\"Consider the possible outcomes: {all_possible_actions_text_for_prompt}.\"\n",
    "        )\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_MAC_SEQ_LENGTH\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        return outputs.hidden_states[-1]  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "# Mô hình Rectified Flow\n",
    "class RectifiedFlowModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        guidance_llm_instance,\n",
    "        num_actions,\n",
    "        flow_embed_dim=256,\n",
    "        projector_hidden_dim=256,\n",
    "        num_attention_heads=4,\n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.guidance_llm_instance = guidance_llm_instance\n",
    "        self.num_actions = num_actions\n",
    "        self.flow_embed_dim = flow_embed_dim\n",
    "        self.projector_hidden_dim = projector_hidden_dim\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # Kích thước từ Guidance LLM\n",
    "        self.hidden_dim_guidance_llm = self.guidance_llm_instance.hidden_dim\n",
    "\n",
    "        # Embedding\n",
    "        self.zt_embed_mlp = nn.Sequential(\n",
    "            nn.Linear(num_actions, flow_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(flow_embed_dim)\n",
    "        )\n",
    "        self.time_embed_mlp = nn.Sequential(\n",
    "            nn.Linear(1, flow_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(flow_embed_dim)\n",
    "        )\n",
    "\n",
    "        # Truy cập W_Q, W_K, W_V\n",
    "        try:\n",
    "            last_guidance_layer_attn = self.guidance_guidance_instance.model.transformer.h[-1].attn\n",
    "            self.guidance_wq = last_guidance_layer_attn.q_wq\n",
    "            self.guidance_wk = last_guidance_layer_attn.k_wk\n",
    "            self.guidance_wv = last_guidance_layer_attn.v_wv\n",
    "            print(\"Successfully accessed Q, K, V projection weights from Guidance LLM.\")\n",
    "            for param in [self.guidance_wq.parameters(), self_guidance_wk.parameters(), self_guidance_wv.parameters()]:\n",
    "                for p in param:\n",
    "                    p.requires_grad = False\n",
    "            self.using_borrowed_weights = True\n",
    "        except AttributeError as e:\n",
    "            print(f\"Warning: Could not access Q, K, V weights ({e}). Using new Linear layers.\")\n",
    "            self.flow_qkv_proj = nn.Linear(self.flow_embed_dim, self.hidden_dim_guidance_llm)\n",
    "            self.guidance_kv_proj = nn.Linear(self.hidden_dim_guidance_llm, self.hidden_dim_guidance_llm)\n",
    "            self.using_borrowed_weights = False\n",
    "\n",
    "        # Cross-attention\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=self.hidden_dim_guidance_guidance_llm,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attn_output_norm = nn.LayerNorm(self.hidden_dim_guidance_llm)\n",
    "        self.attn_output_projection = nn.Linear(self.hidden_dim_guidance_llm, self.flow_embed_dim))\n",
    "\n",
    "        # Projector MLP\n",
    "        projector_modules = []\n",
    "        current_dim = flow_embed_dim\n",
    "        for i in range(projector_layers):\n",
    "            output_dim = projector_hidden_dim if i < projector_layers - 1 else num_actions\n",
    "            projector_modules.append(nn.Linear(current_dim, output_dim))\n",
    "            if i < projector_layers - 1:\n",
    "                projector_modules.extend([\n",
    "                    nn.ReLU(),\n",
    "                    nn.LayerNorm(output_dim),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                ])\n",
    "                current_dim = output_dim\n",
    "        self.projector_mlp = nn.Sequential(*projector_modules)\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=LEARNING_RATE_FLOW_MODEL)\n",
    "\n",
    "    def _project_for_attention(self, tensor, projection_type):\n",
    "        if self.using_borrowed_weights:\n",
    "            if projection_type == 'q': return self.guidance_wq(tensor)\n",
    "            elif projection_type == 'k': return self.guidance_wk(tensor)\n",
    "            elif projection_type == 'v': return self_guidance_wv(tensor)\n",
    "        else:\n",
    "            if projection_type == 'q': return self.flow_qkv_qkv(tensor)\n",
    "            elif projection_type in ['k', 'v']: return self_guidance_kv_proj(tensor)\n",
    "        raise ValueError(f\"Unknown projection_type: {projection_type}\")\n",
    "\n",
    "    def forward(self, z_t, time_t, guidance_llm_last_hidden_states):\n",
    "        batch_size = z_t.shape[0]\n",
    "        # Embedding\n",
    "        h_emb_zt = self.zt_embed_mlp(z_t)\n",
    "        h_emb_t = self.time_embed_mlp(time_t)\n",
    "        flow_tokens_emb = (h_emb_zt + h_emb_t).unsqueeze(1)\n",
    "\n",
    "        # Cross-attention\n",
    "        query = self._project_for_attention(flow_tokens_emb, 'q')\n",
    "        key = self._project_for_attention(guidance_llm_last_hidden_states, 'k')\n",
    "        value = self._project_for_attention(guidance_lm_last_hidden_states, 'v')\n",
    "        attn_output, _ = self.attention_layer(query, key, value, need_weights=False)\n",
    "        attn_output_normalized = self.attn_output_norm(attn_output)\n",
    "        h_attn_zt = self.attn_output_projection(attn_output_normalized.squeeze(1)))\n",
    "\n",
    "        # Projector\n",
    "        vector_field = self.projector_mlp(h_attn_zt)\n",
    "        return vector_field\n",
    "\n",
    "    def train_step(self, z0_batch, z1_batch, time_batch, guidance_llm_hidden_states_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        z_t_batch = time_batch * z1_batch + (1.0 - time_batch) * z0_batch\n",
    "        target_vector_field = z1_batch - z0_batch\n",
    "        predicted_vector_field = self.forward(z_t_batch, time_batch, guidance_llm_hidden_states_batch)\n",
    "        loss = nn.MSELoss()(predicted_vector_field, target_vector_field)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def solve_ode_generate_distribution(self, z0, guidance_llm_last_hidden_states, num_steps=10, return_logits=False):\n",
    "        self.eval()\n",
    "        if z0.ndim == 1: z0 = z0.unsqueeze(0)\n",
    "        if guidance_llm_last_hidden_states.ndim == 2: guidance_llm_last_hidden_states = guidance_llm_last_hidden_states.unsqueeze(0)\n",
    "        batch_size = z0.shape[0]\n",
    "        def ode_func(t, z):\n",
    "            time_t = torch.full((batch_size, 1), t, device=DEVICE)\n",
    "            return self.forward(z, time_t, guidance_llm_last_hidden_states)\n",
    "        t = torch.linspace(0, 1, num_steps, device=DEVICE)\n",
    "        z_t = odeint(ode_func, z0, t, method='rk4')\n",
    "        z1_hat = z_t[-1]\n",
    "        output = z1_hat.squeeze(0) if batch_size == 1 else z1_hat\n",
    "        self.train()\n",
    "        return output if return_logits else torch.softmax(output, dim=-1)\n",
    "\n",
    "# PPO với LoRA\n",
    "def tuning_lm_with_rl(args):\n",
    "    # Cấu hình PPO\n",
    "    config = PPOConfig(\n",
    "        learning_rate=args.rl_learning_rate,\n",
    "        batch_size=4,\n",
    "        mini_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        ppo_epochs=args.ppo_epochs,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    # Tải dataset\n",
    "    dataset_name = args.datasets_dir\n",
    "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    print(f\"Dataset size: {len(train_dataset)}\")\n",
    "\n",
    "    # Tải tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def build_dataset(tokenizer, dataset_name):\n",
    "        \"\"\"Tạo dataset cho PPO\"\"\"\n",
    "        ds = load_dataset(dataset_name, split=\"train\")\n",
    "        repeat_factor = 64 // len(ds) + 1\n",
    "        ds = concatenate_datasets([ds] * repeat_factor).select(range(64))\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            new_examples = {\n",
    "                \"query\": [],\n",
    "                \"input_ids\": [],\n",
    "                \"context\": [],\n",
    "                \"label\": []\n",
    "            }\n",
    "            for context, label in zip(examples[\"context\"], examples[\"label\"]):\n",
    "                query = f\"Analyze: {context}. Provide reasoning without predicting outcome.\"\n",
    "                tokenized_query = tokenizer(\n",
    "                    query,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                new_examples[\"query\"].append(query)\n",
    "                new_examples[\"input_ids\"].append(tokenized_query[\"input_ids\"].squeeze(0))\n",
    "                new_examples[\"context\"].append(context)\n",
    "                new_examples[\"label\"].append(label)\n",
    "            return new_examples\n",
    "\n",
    "        ds = ds.map(preprocess_function, batched=True, remove_columns=ds.column_names)\n",
    "        ds.set_format(type=\"torch\")\n",
    "        return ds\n",
    "\n",
    "    dataset = build_dataset(tokenizer, dataset_name)\n",
    "    print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "    def collator(data):\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "    # Cấu hình LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    )\n",
    "\n",
    "    # Tải mô hình với LoRA\n",
    "    from transformers import AutoModelForCausalLMWithValueHead\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        args.rl_base_model,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        peft_config=lora_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.base_model_prefix = \"model\"\n",
    "    model.generation_config = GenerationConfig.from_pretrained(args.rl_base_model, trust_remote_code=True)\n",
    "    print(f\"Finetune model: {args.rl_base_model}\")\n",
    "\n",
    "    # Khởi tạo PPOTrainer\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=dataset,\n",
    "        data_collator=collator\n",
    "    )\n",
    "\n",
    "    # Khởi tạo ExplanationLLM, GuidanceLLM, RectifiedFlowModel\n",
    "    explanation_llm = ExplanationLLM()\n",
    "    guidance_llm = GuidanceLLM()\n",
    "    flow_model = RectifiedFlowModel(\n",
    "        guidance_llm_instance=guidance_llm,\n",
    "        num_actions=2,  # negative, positive\n",
    "        flow_embed_dim=128,\n",
    "        projector_hidden_dim=128,\n",
    "        num_attention_heads=2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Cấu hình sinh văn bản\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"max_length\": args.output_max_length\n",
    "    }\n",
    "\n",
    "    # Vòng lặp huấn luyện PPO\n",
    "    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        contexts = batch[\"context\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        # Sinh phản hồi từ mô hình PPO (ExplanationLLM với LoRA)\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            return_prompt=False,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        responses = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "        batch[\"response\"] = [r.split(\". \") for r in responses if r.strip()]\n",
    "\n",
    "        # Tính phần thưởng từ RectifiedFlowModel\n",
    "        rewards = []\n",
    "        for context, response, label in zip(contexts, batch[\"response\"], labels):\n",
    "            prob_distributions = []\n",
    "            hidden_states_list = []\n",
    "            z0 = torch.randn(1, 2).to(DEVICE)  # 2 actions\n",
    "            for k in range(len(response) + 1):\n",
    "                current_explanation = response[:k]\n",
    "                hidden_states = guidance_llm.get_last_layer_hidden_states(\n",
    "                    context, current_explanation, \"negative, positive\"\n",
    "                )\n",
    "                flow_dist = flow_model.solve_ode_generate_distribution(\n",
    "                    z0, hidden_states, num_steps=5\n",
    "                )\n",
    "                prob_distributions.append(flow_dist)\n",
    "                hidden_states_list.append(hidden_states)\n",
    "            correct_idx = 1 if label == \"positive\" else 0\n",
    "            reward = sum(\n",
    "                prob_distributions[k][correct_idx] - prob_distributions[k-1][correct_idx]\n",
    "                for k in range(1, len(prob_distributions))\n",
    "            )\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # Bước PPO\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "        # Lưu checkpoint\n",
    "        if args.save_freq and epoch and epoch % args.save_freq == 0:\n",
    "            save_dir = os.path.join(args.output_dir, f\"step_{epoch}\")\n",
    "            ppo_trainer.save_pretrained(save_dir)\n",
    "            print(f\"Saved checkpoint at: {save_dir}\")\n",
    "\n",
    "    # Lưu checkpoint cuối\n",
    "    final_save_dir = os.path.join(args.output_dir, \"step_final\")\n",
    "    ppo_trainer.save_pretrained(final_save_dir)\n",
    "    print(f\"Final checkpoint saved at: {final_save_dir}\")\n",
    "\n",
    "# --- Test tích hợp ---\n",
    "if __name__ == '__main__':\n",
    "    from dataclasses import dataclass\n",
    "\n",
    "    @dataclass\n",
    "    class Args:\n",
    "        rl_learning_rate = 2e-5\n",
    "        ppo_epochs = 10\n",
    "        seed = 100\n",
    "        datasets_dir = \"path/to/your/dataset\"  # Thay bằng dataset thực tế\n",
    "        tokenizer_name = LLM_MODEL_NAME\n",
    "        rl_base_model = LLM_MODEL_NAME\n",
    "        output_max_length = 100\n",
    "        save_freq = 5\n",
    "        output_dir = \"./ppo_checkpoints\"\n",
    "\n",
    "    args = Args()\n",
    "\n",
    "    # Giả lập dataset nếu không có\n",
    "    from datasets import Dataset\n",
    "    sample_data = {\n",
    "        \"context\": [\n",
    "            \"Công ty Alpha ra mắt sản phẩm X, doanh số tăng 30%.\",\n",
    "            \"Công ty Beta bị kiện, cổ phiếu giảm 10%.\"\n",
    "        ],\n",
    "        \"label\": [\"positive\", \"negative\"]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(sample_data)\n",
    "    dataset.save_to_disk(args.datasets_dir)\n",
    "\n",
    "    tuning_lm_with_rl(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77ee1c",
   "metadata": {},
   "source": [
    "# explain mix PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torchdiffeq import odeint # Cần cài đặt: pip install torchdiffeq\n",
    "import numpy as np\n",
    "import random\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset # Để tạo dataset cho PPO\n",
    "import time # Để theo dõi thời gian\n",
    "\n",
    "# Thiết lập seed\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(fix_seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LLM_EXPLANATION_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # Hoặc mô hình bạn muốn tinh chỉnh\n",
    "LLM_GUIDANCE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "MAX_SEQ_LENGTH_GUIDANCE = 512 # Giảm để tiết kiệm bộ nhớ, điều chỉnh nếu cần\n",
    "MAX_SEQ_LENGTH_EXPLANATION_QUERY = 256\n",
    "MAX_NEW_TOKENS_EXPLANATION = 100 # Số token tối đa cho giải thích được tạo\n",
    "LEARNING_RATE_FLOW_MODEL = 2e-4\n",
    "LEARNING_RATE_EXPLANATION_LLM = 1.41e-5 # Thường thấp hơn cho PPO\n",
    "\n",
    "# --- Quantization Config (Nếu cần để tiết kiệm bộ nhớ) ---\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# --- Lớp Guidance LLM (Giữ nguyên từ lần trước, có thể thêm try-except cho load model) ---\n",
    "class GuidanceLLM:\n",
    "    def __init__(self, model_name=LLM_GUIDANCE_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.hidden_dim = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        print(f\"Loading Guidance LLM: {self.model_name}...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16 if DEVICE.type == \"cuda\" else torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                # quantization_config=bnb_config # Bỏ comment nếu dùng quantization\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.model.eval()\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "            # Đảm bảo padding_side cho tokenizer nếu nó sẽ được dùng để tạo input cho model này\n",
    "            # self.tokenizer.padding_side = \"left\" # Thường cho mô hình tự hồi quy\n",
    "\n",
    "            self.hidden_dim = self.model.config.hidden_size\n",
    "            print(f\"Guidance LLM loaded on {DEVICE}. Hidden dim: {self.hidden_dim}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Guidance LLM: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_decision_distribution(self, user_input_text, explanation_sentences_so_far, all_possible_actions):\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        action_log_probs_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for action_text in all_possible_actions:\n",
    "                prompt_base = (\n",
    "                    f\"Given the following information about a company: '{user_input_text}'.\\n\"\n",
    "                    f\"Based on the reasoning: '{current_explanation}'.\\n\"\n",
    "                    f\"Predict the price impact:\"\n",
    "                ) # Không bao gồm action_text trong prompt_base ban đầu\n",
    "                \n",
    "                inputs_base = self.tokenizer(prompt_base, return_tensors=\"pt\", add_special_tokens=True).to(DEVICE)\n",
    "                action_ids_only = self.tokenizer(action_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "                if action_ids_only.shape[1] == 0: # Nếu action_text rỗng hoặc chỉ có special tokens\n",
    "                    action_log_probs_list.append(torch.tensor(-float('inf')).to(DEVICE))\n",
    "                    continue\n",
    "\n",
    "                # Nối action_ids vào input_ids của prompt base để tạo full_input_ids\n",
    "                full_input_ids = torch.cat([inputs_base.input_ids, action_ids_only], dim=1)\n",
    "                \n",
    "                # Giới hạn độ dài để tránh lỗi OOM\n",
    "                if full_input_ids.shape[1] > MAX_SEQ_LENGTH_GUIDANCE:\n",
    "                    # print(f\"Warning: Prompt for action '{action_text}' too long, truncating.\")\n",
    "                    full_input_ids = full_input_ids[:, :MAX_SEQ_LENGTH_GUIDANCE]\n",
    "\n",
    "                # Attention mask cần được tạo lại cho full_input_ids\n",
    "                attention_mask = torch.ones_like(full_input_ids)\n",
    "                \n",
    "                try:\n",
    "                    outputs = self.model(input_ids=full_input_ids, attention_mask=attention_mask)\n",
    "                    logits = outputs.logits # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "                    # Tính log-probability của chuỗi action_ids\n",
    "                    # Chúng ta cần logit của token_i tại vị trí (i-1) của action_ids\n",
    "                    # bắt đầu từ vị trí của token đầu tiên của action_ids trong full_input_ids\n",
    "                    action_start_index_in_logits = inputs_base.input_ids.shape[1] -1 # vị trí logit cho token đầu tiên của action\n",
    "                    \n",
    "                    total_log_prob = torch.tensor(0.0, device=DEVICE)\n",
    "                    for i in range(action_ids_only.shape[1]):\n",
    "                        logit_idx = action_start_index_in_logits + i\n",
    "                        if logit_idx < 0 or logit_idx >= logits.shape[1]: # Kiểm tra biên\n",
    "                            # print(f\"Warning: Logit index {logit_idx} out of bounds for action '{action_text}'.\")\n",
    "                            total_log_prob = torch.tensor(-float('inf')).to(DEVICE)\n",
    "                            break\n",
    "                        \n",
    "                        token_logits = logits[:, logit_idx, :]\n",
    "                        log_probs_vocab = F.log_softmax(token_logits, dim=-1)\n",
    "                        token_to_select = action_ids_only[:, i]\n",
    "                        token_log_prob = log_probs_vocab.gather(1, token_to_select.unsqueeze(-1)).squeeze(-1)\n",
    "                        total_log_prob += token_log_prob\n",
    "                    \n",
    "                    action_log_probs_list.append(total_log_prob.squeeze()) # Squeeze nếu batch_size=1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing action '{action_text}' in get_decision_distribution: {e}\")\n",
    "                    action_log_probs_list.append(torch.tensor(-float('inf')).to(DEVICE))\n",
    "\n",
    "\n",
    "        if not action_log_probs_list or all(lp.item() == -float('inf') for lp in action_log_probs_list):\n",
    "            # print(\"Warning: All action log_probs are -inf. Returning uniform distribution.\")\n",
    "            return torch.ones(len(all_possible_actions), device=DEVICE) / len(all_possible_actions)\n",
    "\n",
    "        log_probs_tensor = torch.stack(action_log_probs_list)\n",
    "        # Vì đây là log_probs, chúng ta có thể dùng softmax trực tiếp\n",
    "        probability_distribution = torch.softmax(log_probs_tensor, dim=0)\n",
    "        return probability_distribution\n",
    "\n",
    "    def get_last_layer_hidden_states(self, user_input_text, explanation_sentences_so_far, all_possible_actions_text_for_prompt):\n",
    "        current_explanation = \" \".join(explanation_sentences_so_far)\n",
    "        prompt = (\n",
    "            f\"Given the following information about a company: '{user_input_text}'.\\n\"\n",
    "            f\"Based on the reasoning: '{current_explanation}'.\\n\"\n",
    "            f\"Consider the possible outcomes: {all_possible_actions_text_for_prompt}.\"\n",
    "        )\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LENGTH_GUIDANCE\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        return outputs.hidden_states[-1]\n",
    "\n",
    "# --- Lớp Rectified Flow Model (Giữ nguyên từ lần trước, đã được điều chỉnh cho Qwen) ---\n",
    "class RectifiedFlowModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        guidance_llm_instance,\n",
    "        num_actions,\n",
    "        flow_embed_dim=256,\n",
    "        projector_hidden_dim=256,\n",
    "        projector_layers=4,\n",
    "        num_attention_heads=4,\n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.guidance_llm_instance = guidance_llm_instance\n",
    "        self.guidance_llm_internal_model = guidance_llm_instance.model\n",
    "        self.num_actions = num_actions\n",
    "        self.flow_embed_dim = flow_embed_dim\n",
    "        self.projector_hidden_dim = projector_hidden_dim\n",
    "        self.projector_layers = projector_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_dim_guidance_llm = self.guidance_llm_instance.hidden_dim\n",
    "\n",
    "        self.zt_embed_mlp = nn.Sequential(\n",
    "            nn.Linear(num_actions, flow_embed_dim), nn.ReLU(), nn.LayerNorm(flow_embed_dim)\n",
    "        )\n",
    "        self.time_embed_mlp = nn.Sequential(\n",
    "            nn.Linear(1, flow_embed_dim), nn.ReLU(), nn.LayerNorm(flow_embed_dim)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            last_guidance_layer_attn = self.guidance_llm_internal_model.transformer.h[-1].attn\n",
    "            self.guidance_wq = last_guidance_layer_attn.q_proj\n",
    "            self.guidance_wk = last_guidance_layer_attn.k_proj\n",
    "            self.guidance_wv = last_guidance_layer_attn.v_proj\n",
    "            for param_group in [self.guidance_wq.parameters(), self.guidance_wk.parameters(), self.guidance_wv.parameters()]:\n",
    "                for p in param_group: p.requires_grad = False\n",
    "            self.using_borrowed_weights = True\n",
    "            print(\"FlowModel: Successfully borrowed Q, K, V projection weights.\")\n",
    "        except AttributeError as e:\n",
    "            print(f\"FlowModel Warning: Could not borrow Q,K,V weights ({e}). Using new Linear layers.\")\n",
    "            self.flow_q_proj_new = nn.Linear(flow_embed_dim, self.hidden_dim_guidance_llm)\n",
    "            self.flow_k_proj_new = nn.Linear(flow_embed_dim, self.hidden_dim_guidance_llm)\n",
    "            self.flow_v_proj_new = nn.Linear(flow_embed_dim, self.hidden_dim_guidance_llm)\n",
    "            self.guidance_k_proj_new = nn.Linear(self.hidden_dim_guidance_llm, self.hidden_dim_guidance_llm)\n",
    "            self.guidance_v_proj_new = nn.Linear(self.hidden_dim_guidance_llm, self.hidden_dim_guidance_llm)\n",
    "            self.using_borrowed_weights = False\n",
    "\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=self.hidden_dim_guidance_llm, num_heads=num_attention_heads, dropout=dropout_rate, batch_first=True\n",
    "        )\n",
    "        self.attn_output_norm = nn.LayerNorm(self.hidden_dim_guidance_llm)\n",
    "        self.attn_output_projection = nn.Linear(self.hidden_dim_guidance_llm, flow_embed_dim)\n",
    "\n",
    "        projector_modules = []\n",
    "        current_dim_proj = flow_embed_dim + num_actions + 1 # h_attn_zt + z_t + time_t\n",
    "        for i in range(projector_layers):\n",
    "            output_dim_proj = projector_hidden_dim if i < projector_layers - 1 else num_actions\n",
    "            projector_modules.append(nn.Linear(current_dim_proj, output_dim_proj))\n",
    "            if i < projector_layers - 1:\n",
    "                projector_modules.extend([\n",
    "                    nn.ReLU(), nn.LayerNorm(output_dim_proj), nn.Dropout(dropout_rate)\n",
    "                ])\n",
    "                current_dim_proj = output_dim_proj + num_actions + 1 # Cho skip connection ở lớp tiếp theo\n",
    "        self.projector_mlp = nn.Sequential(*projector_modules)\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=LEARNING_RATE_FLOW_MODEL)\n",
    "\n",
    "    def _project_for_attention(self, tensor, projection_type):\n",
    "        if self.using_borrowed_weights:\n",
    "            if projection_type == 'q_flow': return self.guidance_wq(tensor)\n",
    "            elif projection_type == 'k_flow': return self.guidance_wk(tensor)\n",
    "            elif projection_type == 'v_flow': return self.guidance_wv(tensor)\n",
    "            elif projection_type == 'k_guidance': return self.guidance_wk(tensor)\n",
    "            elif projection_type == 'v_guidance': return self.guidance_wv(tensor)\n",
    "        else:\n",
    "            if projection_type == 'q_flow': return self.flow_q_proj_new(tensor)\n",
    "            elif projection_type == 'k_flow': return self.flow_k_proj_new(tensor)\n",
    "            elif projection_type == 'v_flow': return self.flow_v_proj_new(tensor)\n",
    "            elif projection_type == 'k_guidance': return self.guidance_k_proj_new(tensor)\n",
    "            elif projection_type == 'v_guidance': return self.guidance_v_proj_new(tensor)\n",
    "        raise ValueError(f\"Unknown projection_type: {projection_type}\")\n",
    "\n",
    "    def forward(self, z_t, time_t, guidance_llm_last_hidden_states):\n",
    "        h_emb_zt = self.zt_embed_mlp(z_t)\n",
    "        h_emb_t = self.time_embed_mlp(time_t)\n",
    "        flow_tokens_emb_stacked = torch.stack([h_emb_zt, h_emb_t], dim=1)\n",
    "\n",
    "        query_input_tensor = flow_tokens_emb_stacked[:, 0:1, :]\n",
    "        q_projected = self._project_for_attention(query_input_tensor, 'q_flow')\n",
    "\n",
    "        k_flow_projected = self._project_for_attention(flow_tokens_emb_stacked, 'k_flow')\n",
    "        k_guidance_projected = self._project_for_attention(guidance_llm_last_hidden_states, 'k_guidance')\n",
    "        combined_keys = torch.cat([k_flow_projected, k_guidance_projected], dim=1)\n",
    "\n",
    "        v_flow_projected = self._project_for_attention(flow_tokens_emb_stacked, 'v_flow')\n",
    "        v_guidance_projected = self._project_for_attention(guidance_llm_last_hidden_states, 'v_guidance')\n",
    "        combined_values = torch.cat([v_flow_projected, v_guidance_projected], dim=1)\n",
    "\n",
    "        attn_output, _ = self.attention_layer(query=q_projected, key=combined_keys, value=combined_values)\n",
    "        attn_output_normalized = self.attn_output_norm(attn_output)\n",
    "        h_attn_zt = self.attn_output_projection(attn_output_normalized.squeeze(1))\n",
    "\n",
    "        projector_input_full = torch.cat([h_attn_zt, z_t, time_t], dim=1)\n",
    "        vector_field = self.projector_mlp(projector_input_full)\n",
    "        return vector_field\n",
    "\n",
    "    def train_step(self, z0_batch, z1_batch, time_batch, guidance_llm_hidden_states_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        z_t_batch = time_batch * z1_batch + (1.0 - time_batch) * z0_batch\n",
    "        target_vector_field = z1_batch - z0_batch\n",
    "        predicted_vector_field = self.forward(z_t_batch, time_batch, guidance_llm_hidden_states_batch)\n",
    "        loss = nn.MSELoss()(predicted_vector_field, target_vector_field)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def solve_ode_generate_distribution(self, z0, guidance_llm_last_hidden_states, num_steps=10, return_logits=False):\n",
    "        self.eval()\n",
    "        if z0.ndim == 1: z0 = z0.unsqueeze(0)\n",
    "        if guidance_llm_last_hidden_states.ndim == 2: guidance_llm_last_hidden_states = guidance_llm_last_hidden_states.unsqueeze(0)\n",
    "        batch_size = z0.shape[0]\n",
    "        def ode_func(t_scalar, z_flat): # t là scalar, z_flat là [B*N]\n",
    "            z = z_flat.view(batch_size, -1) # Reshape về [B, N]\n",
    "            # Tạo time_t_tensor với batch_size\n",
    "            time_t_tensor = torch.full((batch_size, 1), t_scalar.item(), device=z.device, dtype=z.dtype)\n",
    "            vf = self.forward(z, time_t_tensor, guidance_llm_last_hidden_states)\n",
    "            return vf.flatten() # Flatten lại cho ODE solver\n",
    "\n",
    "        t_span = torch.linspace(0, 1, num_steps, device=z0.device)\n",
    "        z_t_solutions = odeint(ode_func, z0.flatten(), t_span, method='rk4') # rk4, dopri5\n",
    "        z1_hat_flat = z_t_solutions[-1]\n",
    "        z1_hat = z1_hat_flat.view(batch_size, -1) # Reshape lại\n",
    "\n",
    "        self.train()\n",
    "        output = z1_hat.squeeze(0) if batch_size == 1 else z1_hat\n",
    "        return output if return_logits else torch.softmax(output, dim=-1)\n",
    "\n",
    "# --- Lớp Explanation LLM với PPO ---\n",
    "class ExplanationLLM_PPO:\n",
    "    def __init__(self, model_name, ppo_config):\n",
    "        self.model_name = model_name\n",
    "        self.ppo_config = ppo_config\n",
    "\n",
    "        # Load model với ValueHead cho PPO\n",
    "        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            # quantization_config=bnb_config, # Nếu dùng quantization\n",
    "            torch_dtype=torch.bfloat16 if DEVICE.type == \"cuda\" else torch.float32,\n",
    "            # peft_config=lora_config, # Nếu dùng LoRA\n",
    "        ).to(DEVICE) # Cần .to(DEVICE) ở đây\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\" # Quan trọng cho PPO với mô hình tự hồi quy\n",
    "\n",
    "        self.ppo_trainer = PPOTrainer(\n",
    "            config=ppo_config,\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer\n",
    "            # optimizer, lr_scheduler có thể được truyền vào nếu bạn muốn tùy chỉnh\n",
    "        )\n",
    "        print(f\"Explanation LLM (for PPO) loaded: {model_name}\")\n",
    "\n",
    "\n",
    "    def generate_explanation_batch(self, query_tensors_list, generation_kwargs):\n",
    "        \"\"\" Tạo giải thích cho một batch các query tensors \"\"\"\n",
    "        # query_tensors_list: list of torch.Tensor (mỗi tensor là tokenized query)\n",
    "        # response_tensors_list = self.ppo_trainer.generate(\n",
    "        #     query_tensors_list,\n",
    "        #     return_prompt=False, # Chỉ trả về phần được sinh ra\n",
    "        #     length_sampler=None, # Hoặc một length sampler nếu muốn độ dài thay đổi\n",
    "        #     **generation_kwargs\n",
    "        # )\n",
    "        # return response_tensors_list\n",
    "        # TRL PPOTrainer.generate mong muốn một list of single query_tensors (không phải batch)\n",
    "        # hoặc một batch tensor duy nhất. Ở đây ta sẽ truyền từng query một.\n",
    "        \n",
    "        response_tensors_list = []\n",
    "        for query_tensor in query_tensors_list:\n",
    "            # Đảm bảo query_tensor có batch_dim = 1\n",
    "            if query_tensor.ndim == 1:\n",
    "                query_tensor = query_tensor.unsqueeze(0)\n",
    "            response = self.ppo_trainer.generate(\n",
    "                query_tensor,\n",
    "                return_prompt=False,\n",
    "                **generation_kwargs\n",
    "            ) # response sẽ là [1, seq_len]\n",
    "            response_tensors_list.append(response.squeeze(0)) # Bỏ batch_dim\n",
    "        return response_tensors_list\n",
    "\n",
    "\n",
    "    def decode_responses(self, response_tensors_list):\n",
    "        return [self.tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors_list]\n",
    "\n",
    "    def train_step(self, query_tensors, response_tensors, reward_tensors):\n",
    "        # query_tensors: list of 1D tensors\n",
    "        # response_tensors: list of 1D tensors\n",
    "        # reward_tensors: list of scalar tensors\n",
    "        # PPOTrainer.step() sẽ xử lý việc tính toán loss và cập nhật model\n",
    "        stats = self.ppo_trainer.step(query_tensors, response_tensors, reward_tensors)\n",
    "        return stats\n",
    "\n",
    "# --- Vòng lặp Huấn luyện Chính ---\n",
    "def main_training_loop(\n",
    "    dataset_list_of_dicts, # list of {'user_input': ..., 'prediction': ..., 'explain': ...}\n",
    "    possible_actions,\n",
    "    num_rounds=2,\n",
    "    epochs_flow_model=20, # Ít hơn để test nhanh\n",
    "    ppo_epochs_per_round=1, # Số lần lặp qua PPO data trong 1 round\n",
    "    ppo_batch_size=4,\n",
    "    gradient_accumulation_steps_ppo=1\n",
    "    ):\n",
    "\n",
    "    print(\"--- Initializing Models ---\")\n",
    "    guidance_llm = GuidanceLLM(model_name=LLM_GUIDANCE_MODEL_NAME)\n",
    "    \n",
    "    ppo_config = PPOConfig(\n",
    "        model_name=LLM_EXPLANATION_MODEL_NAME,\n",
    "        learning_rate=LEARNING_RATE_EXPLANATION_LLM,\n",
    "        batch_size=ppo_batch_size,\n",
    "        mini_batch_size=ppo_batch_size // 2 if ppo_batch_size > 1 else 1, # Phải <= batch_size\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps_ppo,\n",
    "        ppo_epochs=4, # Số PPO epochs mỗi khi gọi step\n",
    "        log_with=\"wandb\", # hoặc None, \"tensorboard\"\n",
    "        # Các siêu tham số PPO khác: kl_penalty, target_kl, clip_epsilon, vf_coef, etc.\n",
    "        remove_unused_columns=False, # Quan trọng nếu dataset có cột thừa\n",
    "    )\n",
    "    explanation_llm_ppo = ExplanationLLM_PPO(LLM_EXPLANATION_MODEL_NAME, ppo_config)\n",
    "\n",
    "    flow_model = RectifiedFlowModel(\n",
    "        guidance_llm_instance=guidance_llm,\n",
    "        num_actions=len(possible_actions),\n",
    "        flow_embed_dim=128, projector_hidden_dim=128, num_attention_heads=2\n",
    "    ).to(DEVICE)\n",
    "    print(\"--- Models Initialized ---\")\n",
    "\n",
    "    generation_kwargs_expl = {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS_EXPLANATION,\n",
    "        \"min_length\": -1, # Không có min_length\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": explanation_llm_ppo.tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": explanation_llm_ppo.tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"\\n=============== ROUND {round_num + 1}/{num_rounds} ===============\")\n",
    "        start_time_round = time.time()\n",
    "\n",
    "        # --- Giai đoạn 1: Tạo Mẫu Dương cho Flow Model ---\n",
    "        print(f\"\\n--- Round {round_num+1}: Generating Positive Samples for Flow Model ---\")\n",
    "        positive_samples_for_flow = [] # list of (z0, z1, guidance_hidden_states)\n",
    "        num_positive_found = 0\n",
    "        for i, data_sample in enumerate(dataset_list_of_dicts):\n",
    "            if i % 50 == 0: print(f\"  Processing data sample {i}/{len(dataset_list_of_dicts)} for positive samples...\")\n",
    "            user_input = data_sample['user_input']\n",
    "            actual_expert_pred = data_sample['prediction']\n",
    "            # Sử dụng giải thích \"vàng\" từ dataset để tạo mẫu dương chất lượng cao ban đầu\n",
    "            expert_explanation_sents = [s.strip() for s in data_sample['explain'].split('.') if s.strip() and len(s.strip()) > 3]\n",
    "            if not expert_explanation_sents: continue\n",
    "\n",
    "            prob_dist_guidance = guidance_llm.get_decision_distribution(\n",
    "                user_input, expert_explanation_sents, possible_actions\n",
    "            )\n",
    "            pred_idx_guidance = torch.argmax(prob_dist_guidance)\n",
    "\n",
    "            if possible_actions[pred_idx_guidance] == actual_expert_pred:\n",
    "                num_positive_found += 1\n",
    "                z1 = prob_dist_guidance.detach().cpu()\n",
    "                z0 = torch.randn_like(z1)\n",
    "                actions_prompt_text = f\"[{', '.join(possible_actions)}]\"\n",
    "                guidance_hs = guidance_llm.get_last_layer_hidden_states(\n",
    "                    user_input, expert_explanation_sents, actions_prompt_text\n",
    "                ).detach().cpu().squeeze(0) # Bỏ batch dim\n",
    "                positive_samples_for_flow.append((z0, z1, guidance_hs))\n",
    "        \n",
    "        if not positive_samples_for_flow:\n",
    "            print(\"No positive samples found for Flow Model in this round. Skipping Flow Model training.\")\n",
    "        else:\n",
    "            print(f\"Found {num_positive_found} positive samples for Flow Model.\")\n",
    "            # --- Giai đoạn 2: Huấn luyện Rectified Flow Model (φ) ---\n",
    "            print(f\"\\n--- Round {round_num+1}: Training Rectified Flow Model (φ) ---\")\n",
    "            flow_model.train()\n",
    "            for epoch in range(epochs_flow_model):\n",
    "                epoch_start_time = time.time()\n",
    "                total_flow_loss = 0\n",
    "                random.shuffle(positive_samples_for_flow) # Xáo trộn mỗi epoch\n",
    "                # Implement batching for flow model training\n",
    "                flow_batch_size = 32\n",
    "                for batch_idx in range(0, len(positive_samples_for_flow), flow_batch_size):\n",
    "                    batch_samples = positive_samples_for_flow[batch_idx : batch_idx + flow_batch_size]\n",
    "                    if not batch_samples: continue\n",
    "\n",
    "                    z0_b = torch.stack([s[0] for s in batch_samples]).to(DEVICE)\n",
    "                    z1_b = torch.stack([s[1] for s in batch_samples]).to(DEVICE)\n",
    "                    guidance_hs_b = torch.stack([s[2] for s in batch_samples]).to(DEVICE)\n",
    "                    time_b = torch.rand(z0_b.shape[0], 1, device=DEVICE) # (batch_size, 1)\n",
    "\n",
    "                    loss = flow_model.train_step(z0_b, z1_b, time_b, guidance_hs_b)\n",
    "                    total_flow_loss += loss * z0_b.shape[0] # Tính loss có trọng số batch size\n",
    "                avg_flow_loss = total_flow_loss / len(positive_samples_for_flow)\n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                print(f\"  Flow Model Epoch {epoch+1}/{epochs_flow_model}, Avg Loss: {avg_flow_loss:.6f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "\n",
    "        # --- Giai đoạn 3: Huấn luyện EXPLANATION LLM (πε) bằng PPO ---\n",
    "        print(f\"\\n--- Round {round_num+1}: Training Explanation LLM (πε) with PPO ---\")\n",
    "        explanation_llm_ppo.model.train() # Đảm bảo model ở chế độ train cho PPO\n",
    "        \n",
    "        # Chuẩn bị dữ liệu cho PPO (lặp qua dataset để tạo queries)\n",
    "        ppo_data_list = [] # List of dicts: {'query': str, 'input_ids': tensor, 'actual_pred': str, 'user_input':str}\n",
    "        for data_sample in dataset_list_of_dicts:\n",
    "            # Query cho Explanation LLM là user_input\n",
    "            query_prompt = f\"Given the context: '{data_sample['user_input']}'. Please analyze the reasoning for the predicted outcome.\"\n",
    "            # Tokenize query\n",
    "            tokenized_query = explanation_llm_ppo.tokenizer(\n",
    "                query_prompt,\n",
    "                truncation=True,\n",
    "                max_length=MAX_SEQ_LENGTH_EXPLANATION_QUERY, # Giới hạn độ dài query\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids.squeeze(0).to(DEVICE) # Bỏ batch dim, lên device\n",
    "\n",
    "            ppo_data_list.append({\n",
    "                \"query_text\": query_prompt, # Để debug\n",
    "                \"query_tensor\": tokenized_query,\n",
    "                \"actual_pred_text\": data_sample['prediction'],\n",
    "                \"user_input_text_for_guidance\": data_sample['user_input'] # Để truyền cho guidance/flow\n",
    "            })\n",
    "        \n",
    "        # PPO training loop\n",
    "        for ppo_epoch_num in range(ppo_epochs_per_round): # Có thể lặp PPO nhiều lần trên cùng 1 batch dữ liệu được tạo ra\n",
    "            print(f\"  PPO Epoch {ppo_epoch_num + 1}/{ppo_epochs_per_round}\")\n",
    "            ppo_epoch_start_time = time.time()\n",
    "            # Xáo trộn dữ liệu PPO\n",
    "            random.shuffle(ppo_data_list)\n",
    "\n",
    "            # Tạo batch cho PPO\n",
    "            for batch_start_idx in range(0, len(ppo_data_list), ppo_config.batch_size):\n",
    "                batch_end_idx = batch_start_idx + ppo_config.batch_size\n",
    "                ppo_batch = ppo_data_list[batch_start_idx:batch_end_idx]\n",
    "                if not ppo_batch: continue\n",
    "\n",
    "                batch_query_tensors = [item['query_tensor'] for item in ppo_batch]\n",
    "                \n",
    "                # 1. Explanation LLM tạo giải thích (response)\n",
    "                response_tensors = explanation_llm_ppo.generate_explanation_batch(\n",
    "                    batch_query_tensors, generation_kwargs_expl\n",
    "                ) # List of 1D tensors\n",
    "\n",
    "                batch_rewards = []\n",
    "                for i in range(len(ppo_batch)):\n",
    "                    query_item = ppo_batch[i]\n",
    "                    generated_response_tensor = response_tensors[i]\n",
    "                    generated_explanation_text = explanation_llm_ppo.tokenizer.decode(generated_response_tensor, skip_special_tokens=True)\n",
    "                    generated_sents = [s.strip() for s in generated_explanation_text.split('.') if s.strip() and len(s.strip()) > 3]\n",
    "                    \n",
    "                    if not generated_sents: # Nếu không tạo được câu nào có nghĩa\n",
    "                        batch_rewards.append(torch.tensor(-1.0, device=DEVICE)) # Phần thưởng phạt nặng\n",
    "                        continue\n",
    "\n",
    "                    # 2. Tính phần thưởng cho giải thích này bằng Flow Model\n",
    "                    # Phần thưởng theo từng câu, sau đó lấy phần thưởng cuối cùng hoặc tổng hợp\n",
    "                    # p_hat_prev_sentence = # p_hat khi chỉ có context (k=0)\n",
    "                    # Để tính p_hat(k=0), ta cần hidden_states khi explanation rỗng\n",
    "                    hs_k0 = guidance_llm.get_last_layer_hidden_states(\n",
    "                        query_item['user_input_text_for_guidance'], [], f\"[{', '.join(possible_actions)}]\"\n",
    "                    ).to(DEVICE)\n",
    "                    z0_phi = torch.randn(len(possible_actions), device=DEVICE)\n",
    "                    p_hat_k_minus_1_dist = flow_model.solve_ode_generate_distribution(z0_phi, hs_k0)\n",
    "                    \n",
    "                    actual_pred_idx = possible_actions.index(query_item['actual_pred_text'])\n",
    "                    p_hat_val_k_minus_1 = p_hat_k_minus_1_dist[actual_pred_idx].item()\n",
    "                    \n",
    "                    final_reward_for_explanation = 0.0\n",
    "                    current_cumulative_sents = []\n",
    "                    for sent_idx, sent_text in enumerate(generated_sents):\n",
    "                        current_cumulative_sents.append(sent_text)\n",
    "                        hs_current_sent = guidance_llm.get_last_layer_hidden_states(\n",
    "                            query_item['user_input_text_for_guidance'], current_cumulative_sents, f\"[{', '.join(possible_actions)}]\"\n",
    "                        ).to(DEVICE)\n",
    "                        \n",
    "                        p_hat_current_sent_dist = flow_model.solve_ode_generate_distribution(z0_phi, hs_current_sent)\n",
    "                        p_hat_val_current_sent = p_hat_current_sent_dist[actual_pred_idx].item()\n",
    "                        \n",
    "                        # Reward cho câu này\n",
    "                        # reward_this_sentence = p_hat_val_current_sent - p_hat_val_k_minus_1\n",
    "                        # p_hat_val_k_minus_1 = p_hat_val_current_sent\n",
    "                        \n",
    "                        # Theo bài báo, reward cho cả giải thích là thay đổi p_hat sau câu cuối cùng\n",
    "                        if sent_idx == len(generated_sents) - 1:\n",
    "                             final_reward_for_explanation = p_hat_val_current_sent - p_hat_val_k_minus_1 # Hoặc p_hat_val_k0\n",
    "                                                                                                       # nếu reward là tổng gain\n",
    "                    batch_rewards.append(torch.tensor(final_reward_for_explanation, device=DEVICE))\n",
    "\n",
    "                # 3. Huấn luyện Explanation LLM bằng PPO step\n",
    "                # PPOTrainer.step nhận list of tensors cho query, response, reward\n",
    "                stats = explanation_llm_ppo.train_step(batch_query_tensors, response_tensors, batch_rewards)\n",
    "                \n",
    "                if stats and 'ppo/loss/total' in stats: # Kiểm tra stats không rỗng\n",
    "                    print(f\"    PPO Batch {batch_start_idx // ppo_config.batch_size +1}, Loss: {stats['ppo/loss/total']:.4f}, Mean Reward: {stats['ppo/mean_scores']:.4f}\")\n",
    "            \n",
    "            ppo_epoch_time = time.time() - ppo_epoch_start_time\n",
    "            print(f\"  PPO Epoch {ppo_epoch_num + 1} finished in {ppo_epoch_time:.2f}s.\")\n",
    "\n",
    "        round_time = time.time() - start_time_round\n",
    "        print(f\"=============== ROUND {round_num + 1} completed in {round_time:.2f}s ===============\")\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "\n",
    "# --- Dữ liệu ví dụ (cần nhiều hơn cho huấn luyện thực tế) ---\n",
    "sample_dataset = [\n",
    "    {'user_input': \"Công ty A: Báo cáo lợi nhuận quý 3 vượt kỳ vọng 15%. CEO công bố kế hoạch mở rộng sang thị trường Châu Á. Chỉ số P/E hiện tại là 12, thấp hơn trung bình ngành (15).\",\n",
    "     'prediction': \"positive\",\n",
    "     'explain': \"Lợi nhuận vượt trội là tín hiệu mạnh. Kế hoạch mở rộng cho thấy tiềm năng tăng trưởng. P/E hấp dẫn so với ngành củng cố quan điểm tích cực.\"},\n",
    "    {'user_input': \"Công ty B: Đối mặt với vụ kiện tụng lớn có thể ảnh hưởng đến 20% doanh thu. Giá cổ phiếu đã giảm 10% trong tuần qua. Ban lãnh đạo chưa có thông báo chính thức.\",\n",
    "     'prediction': \"negative\",\n",
    "     'explain': \"Vụ kiện tụng là rủi ro nghiêm trọng. Sự im lặng của ban lãnh đạo gây thêm bất ổn. Khả năng sụt giảm doanh thu là đáng kể.\"},\n",
    "    {'user_input': \"Công ty C: Ra mắt sản phẩm Z được đánh giá cao nhưng chi phí marketing tăng vọt. Tỷ lệ nợ trên vốn chủ sở hữu ở mức cao (1.5). Thị trường cạnh tranh gay gắt.\",\n",
    "     'prediction': \"negative\", # Hoặc có thể là 'neutral' nếu có\n",
    "     'explain': \"Chi phí marketing lớn và nợ cao tạo áp lực tài chính. Mặc dù sản phẩm mới tốt, rủi ro tổng thể vẫn nghiêng về tiêu cực trong môi trường cạnh tranh này.\"},\n",
    "    # Thêm nhiều mẫu hơn...\n",
    "    {'user_input': \"Công ty D: Vừa nhận được khoản đầu tư lớn từ quỹ uy tín. Công nghệ lõi được cấp bằng sáng chế độc quyền. Đang tuyển dụng nhiều vị trí kỹ sư cao cấp.\",\n",
    "     'prediction': \"positive\",\n",
    "     'explain': \"Khoản đầu tư lớn xác nhận tiềm năng. Bằng sáng chế tạo lợi thế cạnh tranh. Việc tuyển dụng nhân sự giỏi cho thấy sự chuẩn bị cho tăng trưởng.\"},\n",
    "    {'user_input': \"Công ty E: Doanh thu đi ngang trong 3 quý liên tiếp. Thị phần đang bị các đối thủ mới nổi gặm nhấm. Cắt giảm chi phí nhân sự được công bố.\",\n",
    "     'prediction': \"negative\",\n",
    "     'explain': \"Doanh thu trì trệ và mất thị phần là dấu hiệu xấu. Cắt giảm nhân sự cho thấy khó khăn nội tại. Triển vọng không mấy khả quan.\"},\n",
    "]\n",
    "POSSIBLE_ACTIONS = [\"negative\", \"positive\"]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Chạy vòng lặp huấn luyện chính\n",
    "    # Cần nhiều dữ liệu hơn và thời gian huấn luyện dài hơn cho kết quả tốt\n",
    "    main_training_loop(\n",
    "        dataset_list_of_dicts=sample_dataset * 20, # Lặp lại dataset để có nhiều mẫu hơn cho demo\n",
    "        possible_actions=POSSIBLE_ACTIONS,\n",
    "        num_rounds=2,\n",
    "        epochs_flow_model=5, # Giảm để chạy nhanh hơn trong demo\n",
    "        ppo_epochs_per_round=2,\n",
    "        ppo_batch_size=2, # Giảm batch size cho demo\n",
    "        gradient_accumulation_steps_ppo=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126106a",
   "metadata": {},
   "source": [
    "# tuning PPO OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b908974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lm_with_rl(args):\n",
    "    # Khởi tạo script_args từ args\n",
    "    script_args = args\n",
    "    reward_model_name = script_args.reward_model_name\n",
    "    print(\"reward_model_name:\", reward_model_name)\n",
    "\n",
    "    # Đường dẫn dataset\n",
    "    dataset_name = script_args.datasets_dir\n",
    "    print(\"dataset_name:\", dataset_name)\n",
    "\n",
    "    # Cấu hình PPO\n",
    "    config = PPOConfig(\n",
    "        learning_rate=script_args.rl_learning_rate,\n",
    "        # batch_size=script_args.batch_size,\n",
    "        batch_size=4,\n",
    "        # mini_batch_size=script_args.mini_batch_size,\n",
    "        mini_batch_size=2,\n",
    "        # gradient_accumulation_steps=script_args.rl_gradient_accumulation_steps,\n",
    "        gradient_accumulation_steps=1,\n",
    "        ppo_epochs=script_args.ppo_epochs,  \n",
    "        seed=script_args.seed,\n",
    "    )\n",
    "\n",
    "    # Tên mô hình gốc\n",
    "    model_name = script_args.rl_base_model\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # Tải dataset\n",
    "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    print(\"train_dataset size:\", len(train_dataset))\n",
    "\n",
    "    # Cấu hình tham số cho sentiment pipeline\n",
    "    sent_kwargs = {\n",
    "        \"return_all_scores\": True,\n",
    "        \"function_to_apply\": \"none\",\n",
    "        \"batch_size\": 1,\n",
    "        \"truncation\": True\n",
    "    }\n",
    "\n",
    "    # Tải tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded:\", tokenizer.__class__.__name__)\n",
    "\n",
    "    def build_dataset(tokenizer, dataset_name, input_min_text_length=2, input_max_text_length=8):\n",
    "        \"\"\"\n",
    "        Tạo dataset cho huấn luyện PPO.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Tokenizer để mã hóa văn bản.\n",
    "            dataset_name: Tên hoặc đường dẫn dataset.\n",
    "            input_min_text_length: Độ dài tối thiểu của câu hỏi.\n",
    "            input_max_text_length: Độ dài tối đa của câu hỏi.\n",
    "        \n",
    "        Returns:\n",
    "            Dataset đã được xử lý với các cột query và input_ids.\n",
    "        \"\"\"\n",
    "        ds = load_dataset(dataset_name, split=\"train\")\n",
    "        # Giả sử dataset của bạn tên là `ds`, hiện có 6 dòng\n",
    "        repeat_factor = 64 // len(ds) + 1  # Lặp đủ số lần để vượt 64\n",
    "\n",
    "        # Lặp lại nhiều lần rồi cắt còn đúng 64\n",
    "        ds = concatenate_datasets([ds] * repeat_factor)\n",
    "        ds = ds.select(range(64))  # Lấy đúng 64 dòng\n",
    "        original_columns = ds.column_names\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            new_examples = {\n",
    "                \"query\": [],\n",
    "                \"input_ids\": [],\n",
    "            }\n",
    "            # Giả định dataset có cột 'user_input' hoặc 'question'\n",
    "            input_key = \"user_input\" if \"user_input\" in examples else \"question\"\n",
    "            for question in examples[input_key]:\n",
    "                query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "                tokenized_question = tokenizer(\n",
    "                    query,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                new_examples[\"query\"].append(query)\n",
    "                new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"].squeeze(0))\n",
    "            return new_examples\n",
    "\n",
    "        ds = ds.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=original_columns,\n",
    "        )\n",
    "        ds.set_format(type=\"torch\")\n",
    "        return ds\n",
    "\n",
    "    # Tạo dataset\n",
    "    dataset = build_dataset(tokenizer, dataset_name=dataset_name)\n",
    "    print(\"Dataset created with\", len(dataset), \"samples\")\n",
    "\n",
    "    def collator(data):\n",
    "        \"\"\"\n",
    "        Collator để xử lý batch dữ liệu.\n",
    "        \"\"\"\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "    \n",
    "\n",
    "    # Đặt seed để đảm bảo tính tái lập\n",
    "    torch.manual_seed(config.seed)\n",
    "\n",
    "    # Cấu hình LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    )\n",
    "\n",
    "    # Tải mô hình chính với value head\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,  # Đồng bộ với reward model\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "\n",
    "    # Gán thủ công base_model_prefix để tránh lỗi\n",
    "    model.base_model_prefix = \"model\"  # hoặc \"transformer\", tùy thuộc vào tên trong mô hình gốc\n",
    "    # Gán generation_config để tránh lỗi AttributeError\n",
    "    model.generation_config = GenerationConfig.from_pretrained(\n",
    "        \"./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged\"\n",
    "    )\n",
    "    # model.config.return_dict = True\n",
    "\n",
    "    # Tạo generation_config nếu không có\n",
    "    if not hasattr(model, \"generation_config\"):\n",
    "        model.generation_config = GenerationConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    print(\"Finetune model:\", model_name, type(model))\n",
    "\n",
    "    \n",
    "    # Tạo optimizer (nếu dùng Adafactor)\n",
    "    optimizer = None\n",
    "    if script_args.adafactor:\n",
    "        optimizer = Adafactor(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            scale_parameter=False,\n",
    "            relative_step=False,\n",
    "            warmup_init=False,\n",
    "            lr=config.learning_rate,\n",
    "        )\n",
    "\n",
    "    # Khởi tạo PPOTrainer\n",
    "    print(dataset)\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=config,  # Sử dụng args thay vì config\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,  \n",
    "        dataset=dataset,\n",
    "        data_collator=collator,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    # ppo_trainer.train()\n",
    "\n",
    "    # Xác định thiết bị\n",
    "    device = ppo_trainer.accelerator.device\n",
    "    if ppo_trainer.accelerator.num_processes == 1:\n",
    "        device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Tạo sentiment pipeline\n",
    "    sentiment_pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=reward_model_name,\n",
    "        device_map=\"auto\",\n",
    "        # config=reward_model_config,\n",
    "        tokenizer=tokenizer,\n",
    "        # device=device,\n",
    "    )\n",
    "\n",
    "    # Cấu hình tham số sinh văn bản\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    output_min_length = 32\n",
    "    output_max_length = script_args.output_max_length\n",
    "    output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "    # Vòng lặp huấn luyện PPO\n",
    "    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "        # Sinh phản hồi\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            question_tensors,\n",
    "            return_prompt=False,\n",
    "            length_sampler=output_length_sampler,\n",
    "            **generation_kwargs,\n",
    "        )\n",
    "        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "        # Tính điểm thưởng từ reward model\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "        rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n",
    "\n",
    "        # Thực hiện bước PPO\n",
    "        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "        # Lưu checkpoint định kỳ\n",
    "        if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n",
    "            save_dir = os.path.join(script_args.output_dir, f\"step_{epoch}\")\n",
    "            ppo_trainer.save_pretrained(save_dir)\n",
    "            print(f\"Saved checkpoint at: {save_dir}\")\n",
    "\n",
    "    # Lưu checkpoint cuối cùng\n",
    "    final_save_dir = os.path.join(script_args.output_dir, \"step_saved\")\n",
    "    ppo_trainer.save_pretrained(final_save_dir)\n",
    "    print(f\"Final checkpoint saved at: {final_save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
